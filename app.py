import os
import json
import io
import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

from src.visualization import plot_candlestick_with_indicators, plot_corr_heatmap, plot_pca_explained, plot_regression_coeffs, plot_probability_hist, plot_stl_components, plot_acf_pacf, plot_cluster_scatter, plot_factor_portrait, plot_returns_scatter, plot_rolling_corr
from src.analysis.stats import compute_corr_matrix, compute_pca, compute_kmeans, compute_factor_regression, compute_logistic_proba, generate_security_tags
from src.analysis.timeseries import compute_indicators, compute_adx, compute_volatility, compute_garch, compute_stl, compute_acf_pacf, compute_max_drawdown, compute_ann_return, compute_sharpe
from src.conclusion import build_facts, generate_conclusions_with_llm
from src.conclusion import generate_conclusion_text
from src.conclusion import generate_module_advice_text
from src.explanation import (
    explain_trend_adx, explain_volatility_risk, explain_correlation_risk,
    explain_pca_structure, explain_seasonality, explain_prediction_probability,
    explain_factor_regression, explain_clustering, explain_factor_portrait
)
from src.config import load_local_env
from src.report import summarize_fundamentals
from src.report import render_industry_report
from src.report import render_single_report
from src.mapping import load_stock_map, resolve_security, process_stock_map
from src.data_io import ts_to_baostock, fetch_kline_baostock
from src.fundamentals import compute_industry_scoring, compute_symbol_metrics, generate_text_report
from src.data_io import export_financials_single, validate_tushare_token
from advisor import get_or_generate_advisor, generate_followup_reply
from ui import analysis_card, advisor_text, followup, nav_pills
from storage.repository import load_module_result, save_module_result, is_stale, build_payload
from registry.dispatcher import run_selected_modules
from data import find_local_ohlcv, fetch_resolved_df

ROOT = Path(__file__).parent
DATA_DIR = ROOT / "data"
EXPORT_DIR = ROOT / "export"

load_local_env()

st.set_page_config(page_title="æ™ºèƒ½è¯åˆ¸åˆ†æç³»ç»Ÿ", layout="wide")

top_cols = st.columns(5)
top_cols[0].title("æ™ºèƒ½è¯åˆ¸åˆ†æç³»ç»Ÿ")
try:
    from src.data_io import fetch_index_quote_bs, fetch_index_latest_tushare
    @st.cache_data(ttl=3)
    def _load_index_overview(sd: str, ed: str, freq: str):
        index_map = [
            ("000001.SH","ä¸Šè¯æŒ‡æ•°"),
            ("399001.SZ","æ·±è¯æˆæŒ‡"),
            ("000300.SH","æ²ªæ·±300"),
            ("000688.SH","ç§‘åˆ›50")
        ]
        items = []
        for code, name in index_map:
            q = fetch_index_quote_bs(code)
            if not q or q.get("price") is None:
                try:
                    bs_code = ts_to_baostock(code)
                    df_idx = fetch_kline_baostock(bs_code, sd, ed, freq)
                    if not df_idx.empty:
                        df_idx = df_idx.sort_values("date")
                        last = float(df_idx["close"].iloc[-1])
                        prev = float(df_idx["close"].iloc[-2]) if len(df_idx) > 1 else last
                        change = last - prev
                        pct = (change/prev*100.0) if prev != 0 else 0.0
                        ts = pd.to_datetime(df_idx["date"]).iloc[-1]
                        items.append({"index_code": code, "index_name": name, "price": last, "change": change, "pct_change": pct, "timestamp": ts})
                    else:
                        tq = fetch_index_latest_tushare(code, sd, ed)
                        if tq and tq.get("price") is not None:
                            items.append({"index_code": code, "index_name": name, "price": tq.get("price"), "change": tq.get("change"), "pct_change": tq.get("pct_change"), "timestamp": tq.get("timestamp")})
                        else:
                            items.append({"index_code": code, "index_name": name, "price": None, "change": None, "pct_change": None, "timestamp": None})
                except Exception:
                    tq = fetch_index_latest_tushare(code, sd, ed)
                    if tq and tq.get("price") is not None:
                        items.append({"index_code": code, "index_name": name, "price": tq.get("price"), "change": tq.get("change"), "pct_change": tq.get("pct_change"), "timestamp": tq.get("timestamp")})
                    else:
                        items.append({"index_code": code, "index_name": name, "price": None, "change": None, "pct_change": None, "timestamp": None})
            else:
                items.append({"index_code": code, "index_name": name, "price": q.get("price"), "change": q.get("change"), "pct_change": q.get("pct_change"), "timestamp": q.get("timestamp")})
        return pd.DataFrame(items)
    enable_auto = st.checkbox("é¡¶éƒ¨æŒ‡æ•°è‡ªåŠ¨åˆ·æ–°", value=True, key="idx_auto_refresh")
    if enable_auto:
        try:
            st.experimental_autorefresh(interval=3000, key="idx_autorefresh_key")
        except Exception:
            pass
    _today = datetime.now().date()
    _sd = str(_today - timedelta(days=10))
    _ed = str(_today)
    df_idx = _load_index_overview(_sd, _ed, "d")
    for i, (_, row) in enumerate(df_idx.iterrows()):
        if i >= 4: break
        label = row["index_name"]
        val = f"{row['price']:.2f}" if pd.notna(row["price"]) else "N/A"
        delta = f"{row['pct_change']:+.2f}%" if pd.notna(row["pct_change"]) else "N/A"
        top_cols[i+1].metric(label, val, delta=delta)
    if pd.notna(df_idx["timestamp"]).any():
        try:
            ts_show = max([t for t in df_idx["timestamp"].tolist() if t is not None])
            st.caption(f"æ›´æ–°æ—¶é—´: {ts_show}")
        except Exception:
            pass
except Exception:
    pass

# ä¸»åŠŸèƒ½é€‰æ‹© - åœ¨æ ‡é¢˜ä¸‹æ–¹
main_function = st.radio(
    "é€‰æ‹©ä¸»åŠŸèƒ½æ¨¡å—",
    options=["å•ä¸ªè¯åˆ¸åˆ†æ", "è¯åˆ¸å¤šå› å­é‡åŒ–è¯„åˆ†"],
    index=0,
    horizontal=True,
    key="main_function"
)

def render_analysis_card(analysis):
    analysis_card(analysis)

def render_conclusion_item(item):
    if not item: return
    icon = "ğŸ’¡"
    conf = item.get("confidence", 0.5)
    color = "blue"
    if conf >= 0.7: color = "green"
    elif conf <= 0.4: color = "red"
    
    with st.expander(f"{icon} {item.get('title','æœªå‘½åç»“è®º')} (ç½®ä¿¡åº¦: {conf:.2f})", expanded=True):
        st.markdown(f"**æ‘˜è¦**: {item.get('summary','')}")
        if item.get("advice"):
            st.info(f"å»ºè®®: {item.get('advice')}")
        metrics = item.get("metrics", {})
        if metrics:
            st.json(metrics, expanded=False)
            
def render_advisor_text(txt):
    advisor_text(txt)

def render_followup(module: str, inputs: dict, advisor_text_val: str, facts: dict):
    followup(module, inputs, facts)

def get_or_generate_advisor_wrapper(module: str, inputs: dict, facts: dict, time_label: str, symbol: str, industry: str) -> str:
    inputs = dict(inputs or {})
    inputs["time_range"] = time_label
    return get_or_generate_advisor(st.session_state, module, inputs, facts)

with st.sidebar:
    st.header("1. è¡Œä¸šæ˜ å°„ä¸é…ç½®")
    stock_file = st.file_uploader("ä¸Šä¼ è¡Œä¸šè¯åˆ¸æ˜ å°„CSV", type=["csv"], help="æ–‡ä»¶éœ€åŒ…å« symbol, industry åˆ—")
    
    if stock_file is not None:
        try:
            stocks_preview = pd.read_csv(stock_file)
        except Exception:
            try:
                if hasattr(stock_file, "seek"): stock_file.seek(0)
                stocks_preview = pd.read_csv(stock_file, encoding="gbk")
            except Exception:
                stocks_preview = pd.DataFrame()
        st.session_state["stock_map"] = process_stock_map(stocks_preview)
        st.success(f"å·²åŠ è½½æ˜ å°„: {len(stocks_preview)} æ¡")
    
    if "stock_map" not in st.session_state:
        default_map = (ROOT / "stock_industry.csv")
        if default_map.exists():
            st.session_state["stock_map"] = load_stock_map(default_map)
            st.success(f"å·²è‡ªåŠ¨åŠ è½½é»˜è®¤æ˜ å°„: {default_map.name}")
            
    today = datetime.now().date()
    default_start = today - timedelta(days=365)
    start_date = str(default_start)
    end_date = str(today)
    frequency = "d"
    date_col = "date"
    ma_short = 20
    ma_long = 60
    
    st.divider()
    st.header("2. æ•°æ®æºé€‰é¡¹")
    
    with st.expander("æœ¬åœ°æ–‡ä»¶é€‰é¡¹"):
        ohlcv_files = st.file_uploader("ä¸Šä¼ è¡Œæƒ…CSV (æ‰¹é‡)", type=["csv"], accept_multiple_files=True)
        scan_btn = st.button("æ‰«æ data/ohlcv ç›®å½•")
        
        if ohlcv_files:
            items = []
            for f in ohlcv_files:
                items.append((f.name, f))
            if items:
                labels = [it[0] for it in items]
                sel = st.selectbox("é€‰æ‹©ä¸Šä¼ CSVä¸­ç”¨äºå•åªåˆ†æçš„æ–‡ä»¶", options=labels)
                st.session_state["upload_primary_path"] = items[labels.index(sel)][1]
        
        if scan_btn:
            folder = (ROOT / "data" / "ohlcv")
            files = sorted(folder.rglob("*.csv"), key=lambda p: p.stat().st_mtime, reverse=True) if folder.exists() else []
            items = []
            for fp in files:
                items.append((fp.name, str(fp)))
            if items:
                labels = [it[0] for it in items]
                values = [it[1] for it in items]
                sel = st.selectbox("é€‰æ‹©æœ¬åœ°CSV", options=labels)
                st.session_state["local_csv_path"] = values[labels.index(sel)]
    
    st.divider()
    st.header("3. æ‰¹é‡å¤„ç†ä¸å¯¼å‡º")
    
    with st.expander("æ‰¹é‡å¯¼å‡ºé€‰é¡¹"):
        industries_all = []
        if "stock_map" in st.session_state:
            industries_all = sorted(set(st.session_state["stock_map"]["industry"].dropna().tolist()))
        
        if industries_all:
            industries_sel = st.multiselect("é€‰æ‹©æ‰¹é‡å¯¼å‡ºè¡Œä¸š", options=industries_all)
            export_dir = st.text_input("å¯¼å‡ºç›®å½• (OHLCV)", value=str((ROOT / "data" / "ohlcv").resolve()))
            export_start_date = st.date_input("å¯¼å‡ºå¼€å§‹æ—¥æœŸ", value=default_start, key="export_start_date")
            export_end_date = st.date_input("å¯¼å‡ºç»“æŸæ—¥æœŸ", value=today, key="export_end_date")
            custom_inds = st.text_input("è‡ªå®šä¹‰è¡Œä¸šåç§°ï¼ˆé€—å·åˆ†éš”ï¼‰", value="")
            do_export = st.button("æŒ‰è¡Œä¸šæ‰¹é‡å¯¼å‡ºè¡Œæƒ…CSV")
            if do_export and industries_sel:
                from src.data_io import export_batch
                inds_final = list(industries_sel)
                if custom_inds.strip():
                    inds_final.extend([s.strip() for s in custom_inds.split(",") if s.strip()])
                sd_exp = str(export_start_date)
                ed_exp = str(export_end_date)
                export_batch(st.session_state["stock_map"], inds_final, sd_exp, ed_exp, Path(export_dir))
                st.success(f"æ‰¹é‡å¯¼å‡ºå®Œæˆï¼š{', '.join(inds_final)}")

        funda_dir = st.text_input("è´¢æŠ¥ç›®å½•", value=str((ROOT / "data" / "fundamentals").resolve()))
        do_funda = st.button("æ‰¹é‡é‡‡é›†è´¢æŠ¥(å››è¡¨)")
        if do_funda and "stock_map" in st.session_state:
            from src.data_io import export_financials_single, validate_tushare_token
            st.session_state["funda_dir"] = funda_dir
            ok = validate_tushare_token()
            if not ok:
                st.error("Tushare Token æ— æ•ˆæˆ–æƒé™ä¸è¶³")
            else:
                try:
                    df_map = st.session_state.get("stock_map")
                    industry = st.session_state.get("selected_industry", industries_all[0] if industries_all else "")
                    if df_map is not None and industry:
                        df_ind = df_map[df_map["industry"] == industry]
                        symbols = df_ind["symbol"].dropna().astype(str).tolist() if "symbol" in df_ind.columns else []
                        if not symbols:
                            st.warning("è¯¥è¡Œä¸šä¸‹æ²¡æœ‰è¯åˆ¸æ•°æ®")
                        else:
                            prog = st.progress(0.0)
                            stat = st.empty()
                            succ = 0; fail = 0
                            total = len(symbols)
                            sd_fmt = start_date.replace("-",""); ed_fmt = end_date.replace("-","")
                            for i, ts_code in enumerate(symbols, start=1):
                                try:
                                    stat.markdown(f"é‡‡é›†ä¸­: {ts_code} ({i}/{total})")
                                    export_financials_single(ts_code, sd_fmt, ed_fmt, Path(funda_dir), industry)
                                    succ += 1
                                except Exception as e:
                                    fail += 1
                                    stat.markdown(f"é‡‡é›†å¤±è´¥: {ts_code}ï¼ŒåŸå› : {e}")
                                prog.progress(i/total)
                            stat.markdown(f"å®Œæˆ: æˆåŠŸ {succ} / å¤±è´¥ {fail} / æ€»è®¡ {total}")
                            if fail == 0:
                                st.success("è´¢æŠ¥é‡‡é›†å®Œæˆ")
                            else:
                                st.warning("éƒ¨åˆ†è¯åˆ¸é‡‡é›†ä¸­å‡ºç°å¤±è´¥ï¼Œè¯·æ£€æŸ¥Tokenæƒé™æˆ–ç½‘ç»œçŠ¶æ€")
                    else:
                        st.error("è¯·å…ˆé€‰æ‹©è¡Œä¸š")
                except Exception as e:
                    st.error(f"è´¢æŠ¥é‡‡é›†å¤±è´¥: {e}")
        
        do_daily_basic = st.button("é‡‡é›†æ—¥é¢‘ä¼°å€¼(daily_basic)")
        if do_daily_basic and "stock_map" in st.session_state:
            from src.data_io import export_daily_basic_batch, validate_tushare_token
            st.session_state["funda_dir"] = funda_dir
            ok = validate_tushare_token()
            if not ok:
                st.error("Tushare Token æ— æ•ˆæˆ–æƒé™ä¸è¶³")
            else:
                try:
                    df_map = st.session_state.get("stock_map")
                    industry = st.session_state.get("selected_industry", industries_all[0] if industries_all else "")
                    if df_map is not None and industry:
                        export_daily_basic_batch(df_map, industry, start_date.replace("-",""), end_date.replace("-",""), Path(funda_dir))
                        st.success("daily_basic é‡‡é›†å®Œæˆ")
                    else:
                        st.error("è¯·å…ˆé€‰æ‹©è¡Œä¸š")
                except Exception as e:
                    st.error(f"daily_basic é‡‡é›†å¤±è´¥: {e}")
        
        do_validate_pepb = st.button("éªŒè¯ä¼°å€¼æ•°æ®(pe/pb)")
        if do_validate_pepb and "stock_map" in st.session_state:
            industry = st.session_state.get("ind_sel_right", industries_all[0] if industries_all else "")
            if industry:
                df_map = st.session_state.get("stock_map")
                df_ind = df_map[df_map["industry"] == industry] if df_map is not None else pd.DataFrame()
                syms = df_ind["symbol"].dropna().astype(str).tolist() if "symbol" in df_ind.columns else []
                rows = []
                for sym in syms:
                    try:
                        fp = Path(funda_dir) / sym.replace(".","_") / "fina_indicator.csv"
                        fp_db = Path(funda_dir) / sym.replace(".","_") / "daily_basic.csv"
                        pe_col = None; pb_col = None; pe_val = None; pb_val = None; src = "ç¼ºå¤±"
                        if fp_db.exists():
                            try:
                                db = pd.read_csv(fp_db)
                            except Exception:
                                db = pd.read_csv(fp_db, encoding="gbk")
                            db = db.sort_values(["trade_date"]) if "trade_date" in db.columns else db
                            pe_cols_db = [c for c in ["pe","pe_ttm"] if c in db.columns]
                            pb_cols_db = [c for c in ["pb","pb_mrq"] if c in db.columns]
                            if pe_cols_db:
                                pe_col = pe_cols_db[0]
                                ser_pe = pd.to_numeric(db[pe_col], errors="coerce").dropna()
                                pe_val = float(ser_pe.iloc[-1]) if not ser_pe.empty else None
                            if pb_cols_db:
                                pb_col = pb_cols_db[0]
                                ser_pb = pd.to_numeric(db[pb_col], errors="coerce").dropna()
                                pb_val = float(ser_pb.iloc[-1]) if not ser_pb.empty else None
                            src = "daily_basic"
                        if (pe_val is None or pb_val is None) and fp.exists():
                            try:
                                fi = pd.read_csv(fp)
                            except Exception:
                                fi = pd.read_csv(fp, encoding="gbk")
                            fi = fi.sort_values(["end_date","ann_date"]) if "end_date" in fi.columns and "ann_date" in fi.columns else fi
                            pe_cols = [c for c in ["pe","pe_ttm","pe_basic","pe_circ"] if c in fi.columns]
                            pb_cols = [c for c in ["pb","pb_mrq"] if c in fi.columns]
                            if pe_val is None and pe_cols:
                                pe_col = pe_cols[0]
                                ser_pe = pd.to_numeric(fi[pe_col], errors="coerce").dropna()
                                pe_val = float(ser_pe.iloc[-1]) if not ser_pe.empty else None
                                src = "fina_indicator"
                            if pb_val is None and pb_cols:
                                pb_col = pb_cols[0]
                                ser_pb = pd.to_numeric(fi[pb_col], errors="coerce").dropna()
                                pb_val = float(ser_pb.iloc[-1]) if not ser_pb.empty else None
                                src = "fina_indicator"
                        rows.append({"symbol": sym, "files": "å·²é‡‡é›†" if src != "ç¼ºå¤±" else "ç¼ºå¤±", "source": src, "pe_col": pe_col, "pe_val": pe_val, "pb_col": pb_col, "pb_val": pb_val})
                    except Exception:
                        rows.append({"symbol": sym, "files": "å¼‚å¸¸", "source": None, "pe_col": None, "pe_val": None, "pb_col": None, "pb_val": None})
                df_check = pd.DataFrame(rows).set_index("symbol")
                st.subheader("ä¼°å€¼æ•°æ®éªŒè¯")
                st.dataframe(df_check)
                n_missing = int((df_check["pe_val"].isna()).sum() + (df_check["pb_val"].isna()).sum())
                if n_missing > 0:
                    st.warning("éƒ¨åˆ†æ ‡çš„ä¼°å€¼ç¼ºå¤±ï¼Œå»ºè®®é‡‡é›† daily_basic æˆ–åœ¨è´¢æŠ¥æ¥å£æ˜¾å¼åŠ å…¥ pe_ttm/pb_mrq å­—æ®µ")
        
        do_report = st.button("è¡Œä¸šåŸºæœ¬é¢æ¦‚è§ˆ(HTML)")
        if do_report and "stock_map" in st.session_state:
            industry = st.session_state.get("ind_sel_right", industries_all[0] if industries_all else "")
            if industry:
                charts = []
                ind_dir = EXPORT_DIR / industry
                if ind_dir.exists():
                    for fp in ind_dir.glob("*.png"):
                        charts.append(str(fp))
                conclusions_paths = []
                for fp in EXPORT_DIR.glob(f"*_{industry}_conclusions.json"):
                    conclusions_paths.append(str(fp))
                symbols = []
                if "stock_map" in st.session_state:
                    sm = st.session_state["stock_map"]
                    df_ind_map = sm[sm["industry"] == industry]
                    symbols = df_ind_map["symbol"].dropna().astype(str).tolist() if "symbol" in df_ind_map.columns else []
                out_fp = render_industry_report(industry, symbols, Path(funda_dir), EXPORT_DIR, charts, conclusions_paths)
                st.success(f"è¡Œä¸šæŠ¥å‘Šå·²ç”Ÿæˆ: {out_fp}")
    
    st.divider()
    st.header("4. å¤§æ¨¡å‹é…ç½®")
    llm_model = st.text_input("LLMæ¨¡å‹", value=os.environ.get("LLM_MODEL", "gpt-4"), key="llm_model")
    llm_endpoint = st.text_input("LLMç«¯ç‚¹", value=os.environ.get("LLM_ENDPOINT", ""), key="llm_endpoint")
    llm_api_key = st.text_input("APIå¯†é’¥", value=os.environ.get("LLM_API_KEY", ""), type="password", key="llm_api_key")
    if llm_model:
        os.environ["LLM_MODEL"] = llm_model
    if llm_endpoint:
        os.environ["LLM_ENDPOINT"] = llm_endpoint
    if llm_api_key:
        os.environ["LLM_API_KEY"] = llm_api_key
    
    st.divider()
    st.header("5. æ˜¾ç¤ºé€‰é¡¹")
    st.checkbox("æ˜¾ç¤ºç»¼åˆç»“è®ºï¼ˆ10æ¡ï¼‰", value=st.session_state.get("show_conclusions", False), key="show_conclusions")
    
    st.divider()
    if st.button("æ¸…ç©ºä¼šè¯ç¼“å­˜"):
        for k in ["df_source_left", "df_source_right", "resolved_df", "last_fetch_key", "module_cache", "advisor_extras_done", "pivot_close"]:
            if k in st.session_state:
                del st.session_state[k]
        st.success("å·²æ¸…ç©ºç¼“å­˜")

# åˆ›å»ºå¸ƒå±€ï¼šæ ¹æ®æ¨¡å¼åˆ‡æ¢ä»…æ˜¾ç¤ºä¸€ä¸ªå·¥ä½œåŒº
ratio = [1, 0.0001] if main_function == "å•ä¸ªè¯åˆ¸åˆ†æ" else [0.0001, 1]
left_col, right_col = st.columns(ratio)

# å·¦ä¾§æ¨¡å—ï¼šå•ä¸ªè¯åˆ¸åˆ†æ
with left_col:
    if main_function == "å•ä¸ªè¯åˆ¸åˆ†æ":
        st.header("å•ä¸ªè¯åˆ¸åˆ†æ")
        # è¯åˆ¸æŸ¥è¯¢
        query = st.text_input("è¾“å…¥è¯åˆ¸ä»£ç æˆ–åç§°", value="", placeholder="ä¾‹å¦‚: 600006 æˆ– æ™‹è¥¿è½¦è½´", key="query_left")
        
        col1, col2 = st.columns(2)
        with col1:
            sd_local = st.date_input("å¼€å§‹æ—¥æœŸ", value=datetime.now()-timedelta(days=365), key="sd_left")
        with col2:
            ed_local = st.date_input("ç»“æŸæ—¥æœŸ", value=datetime.now(), key="ed_left")
        
        # æ‰§è¡Œå•ä¸ªè¯åˆ¸åˆ†æï¼ˆè‡ªåŠ¨è§¦å‘ï¼Œæ— éœ€æŒ‰é’®ï¼‰
        if query:
            df = None
            local_csv_path = st.session_state.get("local_csv_path")
            upload_primary_path = st.session_state.get("upload_primary_path")
            
            if local_csv_path:
                try:
                    df = pd.read_csv(local_csv_path)
                except Exception:
                    df = pd.read_csv(local_csv_path, encoding="gbk")
            elif upload_primary_path is not None:
                try:
                    df = pd.read_csv(upload_primary_path)
                except Exception:
                    if hasattr(upload_primary_path, "seek"):
                        upload_primary_path.seek(0)
                    df = pd.read_csv(upload_primary_path, encoding="gbk")
            else:
                # å…ˆæŸ¥æœ¬åœ°ï¼Œå†ç”¨ç¬¬ä¸‰æ–¹æ¥å£
                df_map = st.session_state.get("stock_map")
                resolved = resolve_security(df_map, query) if df_map is not None else None
                symbol_try = None
                industry_try = "æœªçŸ¥"
                name_try = ""
                if resolved:
                    symbol_try, industry_try, name_try = resolved
                    st.success(f"å·²è¯†åˆ«: {name_try} ({symbol_try}) - {industry_try}")
                else:
                    from src.data_io import normalize_ts_code
                    symbol_try = normalize_ts_code(query)
                if symbol_try:
                    local_path = find_local_ohlcv(ROOT, symbol_try)
                    if local_path:
                        try:
                            df = pd.read_csv(local_path)
                        except Exception:
                            df = pd.read_csv(local_path, encoding="gbk")
                        st.info(f"ä½¿ç”¨æœ¬åœ°æ•°æ®: {Path(local_path).name}")
                        try:
                            latest_dt = pd.to_datetime(df[date_col], errors="coerce").max()
                            latest_txt = latest_dt.strftime("%Y-%m-%d") if pd.notna(latest_dt) else "æœªçŸ¥"
                        except Exception:
                            latest_txt = "æœªçŸ¥"
                        st.caption(f"æœ¬åœ°æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_txt}")
                        do_refetch = st.button("é‡æ–°é‡‡é›†æœ€æ–°è¡Œæƒ…", key=f"refetch_{symbol_try}")
                        if do_refetch:
                            with st.spinner(f"æ­£åœ¨é‡æ–°é‡‡é›† {symbol_try} è¡Œæƒ…æ•°æ®..."):
                                df_rt = fetch_resolved_df(st.session_state, symbol_try, industry_try, str(sd_local), str(ed_local), frequency)
                                if not df_rt.empty:
                                    df = df_rt
                                    st.success(f"å·²æ›´æ–°è¡Œæƒ…æ•°æ® ({len(df)} æ¡)")
                                else:
                                    st.warning("é‡æ–°é‡‡é›†å¤±è´¥æˆ–æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¥æœŸæˆ–ä»£ç ")
                    else:
                        with st.spinner(f"æ­£åœ¨æ‹‰å– {symbol_try} è¡Œæƒ…æ•°æ®..."):
                            df_rt = fetch_resolved_df(st.session_state, symbol_try, industry_try, str(sd_local), str(ed_local), frequency)
                            if not df_rt.empty:
                                df = df_rt
                                st.success(f"è¡Œæƒ…æ‹‰å–æˆåŠŸ ({len(df)} æ¡)")
                            else:
                                st.warning("æœªè·å–åˆ°è¡Œæƒ…æ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¥æœŸæˆ–ä»£ç ")
                else:
                    st.warning(f"æœªè¯†åˆ«åˆ°è¯åˆ¸: {query}")
            
            if df is not None and not df.empty:
                st.session_state["df_source_left"] = df
                if resolved:
                    st.session_state["symbol_left"] = symbol_try
                    st.session_state["industry_left"] = industry_try
                    st.session_state["name_left"] = name_try
                else:
                    if "symbol" in df.columns:
                        symbol_extracted = str(df["symbol"].dropna().iloc[0])
                        st.session_state["symbol_left"] = symbol_extracted
                        st.session_state["industry_left"] = "æœªçŸ¥"
                        st.session_state["name_left"] = query
            else:
                st.error("æ— æ³•åŠ è½½æ•°æ®")
        
        # å¦‚æœæœ‰æ•°æ®ï¼Œæ˜¾ç¤ºåˆ†ææ¨¡å—
        if "df_source_left" in st.session_state and st.session_state["df_source_left"] is not None:
            df = st.session_state["df_source_left"]
            symbol = st.session_state.get("symbol_left", "")
            industry = st.session_state.get("industry_left", "")
            name = st.session_state.get("name_left", "")
            
            df[date_col] = pd.to_datetime(df[date_col])
            df = df.sort_values(date_col)
            
            # ç¡®å®šsymbol
            if not symbol and "symbol" in df.columns:
                symbols_in_df = [str(x) for x in df["symbol"].dropna().unique().tolist()]
                if symbols_in_df:
                    symbol = symbols_in_df[0]
            
            df_symbol = df[df["symbol"] == symbol] if symbol and "symbol" in df.columns else df
            
            if not df_symbol.empty:
                modules = ["Kçº¿ä¸æŒ‡æ ‡", "ç›¸å…³æ€§åˆ†æ", "PCAåˆ†æ", "æ³¢åŠ¨æ€§åˆ†æ", "å­£èŠ‚æ€§åˆ†æ", "é£é™©-æ”¶ç›Šèšç±»åˆ†æ", "åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ", "æ¶¨è·Œæ¦‚ç‡åˆ†æ"]
                sel_multi = st.multiselect("é€‰æ‹©è¦æ‰§è¡Œçš„åˆ†ææ¨¡å—", options=modules, default=["Kçº¿ä¸æŒ‡æ ‡"], key="module_batch_sel_left")
                run_selected_btn = st.button("å¼€å§‹åˆ†æ(ä»…é€‰ä¸­æ¨¡å—)", key="run_selected_left", type="primary")
                if run_selected_btn and sel_multi:
                    st.session_state.setdefault("module_status_left", {})
                    with st.spinner("åˆ†ææ‰§è¡Œä¸­..."):
                        status = run_selected_modules(st.session_state, ROOT, symbol, industry, df_symbol, date_col, sd_local, ed_local, frequency, ma_short, ma_long, sel_multi)
                        for mod, s in status.items():
                            st.session_state["module_status_left"][mod] = s
                    st.success("é€‰ä¸­æ¨¡å—åˆ†æå®Œæˆ")
                params_now = {"fetch_key": st.session_state.get("last_fetch_key"), "ma_short": ma_short, "ma_long": ma_long, "frequency": frequency}
                st.session_state.setdefault("module_status_left", {})
                for mod in modules:
                    if mod not in st.session_state["module_status_left"]:
                        cached_payload = load_module_result(ROOT, symbol, mod)
                        if not cached_payload:
                            st.session_state["module_status_left"][mod] = {"status": "æœªåˆ†æ", "time": None, "fresh": None}
                        else:
                            stale = is_stale(cached_payload, params_now)
                            st.session_state["module_status_left"][mod] = {"status": "éœ€é‡æ–°åˆ†æ" if stale else "å·²å®Œæˆ", "time": cached_payload.get("timestamp"), "fresh": not stale}
                with st.expander("æ¨¡å—çŠ¶æ€", expanded=False):
                    for mod in modules:
                        s = st.session_state["module_status_left"].get(mod, {"status": "æœªåˆ†æ", "time": None, "fresh": None})
                        lbl = f"{mod} | çŠ¶æ€: {s['status']}"
                        if s["time"]:
                            lbl += f" | ä¸Šæ¬¡åˆ†æ: {s['time']}"
                        if s["status"] == "éœ€é‡æ–°åˆ†æ":
                            lbl += " | åŸå› : å‚æ•°æˆ–æ•°æ®å˜æ›´"
                        st.caption(lbl)
                module_sel = st.pills("é€‰æ‹©åˆ†ææ¨¡å—", modules, default="Kçº¿ä¸æŒ‡æ ‡", key="module_sel_left_pills")
                
                ind = compute_indicators(df_symbol.set_index(date_col), ma_short=ma_short, ma_long=ma_long)
                if "Kçº¿ä¸æŒ‡æ ‡" in st.session_state.get("mod_cache_left", {}):
                    ind = st.session_state["mod_cache_left"]["Kçº¿ä¸æŒ‡æ ‡"].get("ind", ind)
                    adx_prefetch = st.session_state["mod_cache_left"]["Kçº¿ä¸æŒ‡æ ‡"].get("adx", None)
                
                # æ„å»ºäº‹å®æ•°æ®
                facts = build_facts(df_symbol, ind, None, None, symbol, industry)
                
                if st.session_state.get("show_conclusions", False):
                    st.divider()
                    st.subheader("ç»¼åˆç»“è®ºï¼ˆ10æ¡ï¼‰")
                    try:
                        conclusions = generate_conclusions_with_llm(facts, [])
                    except Exception:
                        conclusions = []
                    if conclusions:
                        for item in conclusions[:10]:
                            render_conclusion_item(item)
                    else:
                        st.info("ç»¼åˆç»“è®ºç”Ÿæˆå¤±è´¥æˆ–æ•°æ®ä¸è¶³")
                
                # æ ¹æ®æ¨¡å—æ˜¾ç¤ºä¸åŒå†…å®¹ï¼ˆåŠ å…¥æ¨¡å—çº§ç¼“å­˜ï¼Œåˆ‡æ¢ä¸é‡æ–°è®¡ç®—ï¼‰
                mod_cache = st.session_state.setdefault("mod_cache_left", {})
                if module_sel == "ç›¸å…³æ€§åˆ†æ" and "ç›¸å…³æ€§åˆ†æ" not in mod_cache:
                    payload = load_module_result(ROOT, symbol, "ç›¸å…³æ€§åˆ†æ")
                    if payload and payload.get("data", {}).get("corr"):
                        corr_df = pd.DataFrame(payload["data"]["corr"])
                        fig_corr = plot_corr_heatmap(corr_df)
                        mod_sum = f"è¡Œä¸šç›¸å…³æ€§åˆ†æ: å¹³å‡ç›¸å…³æ€§: {corr_df.mean().mean():.3f}"
                        advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                        mod_cache["ç›¸å…³æ€§åˆ†æ"] = {"corr": corr_df, "fig_corr": fig_corr, "advisor_inputs": advisor_inputs}
                if module_sel == "Kçº¿ä¸æŒ‡æ ‡" and "Kçº¿ä¸æŒ‡æ ‡" not in mod_cache:
                    payload = load_module_result(ROOT, symbol, "Kçº¿ä¸æŒ‡æ ‡")
                    d = payload["data"] if payload else {}
                    ind_d = d.get("ind"); adx_d = d.get("adx")
                    if ind_d and adx_d:
                        ind_cached = pd.DataFrame(ind_d["data"], index=pd.to_datetime(ind_d["index"]), columns=ind_d["columns"])
                        try:
                            idx_try = pd.to_datetime(adx_d["index"], errors="coerce")
                            if hasattr(idx_try, "isna") and idx_try.isna().any():
                                tail_idx = pd.to_datetime(df_symbol[date_col]).iloc[-len(adx_d["values"]):]
                                idx = tail_idx
                            else:
                                idx = idx_try
                        except Exception:
                            idx = pd.to_datetime(df_symbol[date_col]).iloc[-len(adx_d["values"]):]
                        n = min(len(adx_d["values"]), len(idx))
                        adx_cached = pd.Series(list(adx_d["values"])[-n:], index=idx[-n:])
                        mod_cache["Kçº¿ä¸æŒ‡æ ‡"] = {"ind": ind_cached, "adx": adx_cached}
                if module_sel == "Kçº¿ä¸æŒ‡æ ‡":
                    st.subheader("Kçº¿ä¸æŒ‡æ ‡")
                    end_dt = pd.to_datetime(df_symbol[date_col]).max()
                    ranges_labels = ["ä¸€å‘¨", "ä¸€ä¸ªæœˆ", "ä¸‰ä¸ªæœˆ", "å…­ä¸ªæœˆ", "ä¸€å¹´", "ä¸‰å¹´", "äº”å¹´", "å…¨éƒ¨"]
                    ranges_days = {
                        "ä¸€å‘¨": 7,
                        "ä¸€ä¸ªæœˆ": 30,
                        "ä¸‰ä¸ªæœˆ": 90,
                        "å…­ä¸ªæœˆ": 180,
                        "ä¸€å¹´": 365,
                        "ä¸‰å¹´": 365*3,
                        "äº”å¹´": 365*5
                    }
                    
                    sel_lbl = st.selectbox("æŸ¥çœ‹åŒºé—´", ranges_labels, index=4)
                    if sel_lbl == "å…¨éƒ¨":
                        df_slice = df_symbol
                        ind_slice = ind
                    else:
                        start_dt = end_dt - timedelta(days=ranges_days[sel_lbl])
                        df_slice = df_symbol[df_symbol[date_col] >= start_dt]
                        if df_slice.empty:
                            df_slice = df_symbol
                        ind_slice = ind.loc[df_slice[date_col].values] if not df_slice.empty else ind
                    
                    st.subheader("ä¸‰ç§’å¿«è§ˆ")
                    try:
                        ma_slope = float(ind_slice["SMA"].diff().dropna().iloc[-1]) if "SMA" in ind_slice.columns else None
                        ema_slope = float(ind_slice["EMA"].diff().dropna().iloc[-1]) if "EMA" in ind_slice.columns else None
                        rsi_last = float(ind_slice["RSI"].iloc[-1]) if "RSI" in ind_slice.columns else None
                    except Exception:
                        ma_slope = None; ema_slope = None; rsi_last = None
                    trend = "å¤šå¤´" if ((ma_slope or 0) > 0) or ((ema_slope or 0) > 0) else ("ç©ºå¤´" if ((ma_slope or 0) < 0 and (ema_slope or 0) < 0) else "éœ‡è¡")
                    rsi_tag = "è¶…ä¹°" if (rsi_last is not None and rsi_last >= 70) else ("è¶…å–" if (rsi_last is not None and rsi_last <= 30) else "ä¸­æ€§")
                    try:
                        adx_series = adx_prefetch if 'adx_prefetch' in locals() and adx_prefetch is not None else compute_adx(df_symbol)
                        adx_last = float(adx_series.iloc[-1]) if adx_series is not None and not adx_series.empty else None
                    except Exception:
                        adx_last = None
                    adx_tag = "å¼±" if (adx_last is not None and adx_last < 20) else ("ä¸­" if (adx_last is not None and adx_last < 40) else ("å¼º" if (adx_last is not None and adx_last < 60) else ("æå¼º" if adx_last is not None else "N/A")))
                    try:
                        r = df_slice["close"].pct_change(fill_method=None).dropna()
                        vol_ann = float(r.std() * np.sqrt(252)) if not r.empty else None
                    except Exception:
                        vol_ann = None
                    vol_tag = "ä½" if (vol_ann is not None and vol_ann < 0.20) else ("ä¸­" if (vol_ann is not None and vol_ann < 0.35) else ("é«˜" if (vol_ann is not None) else "N/A"))
                    c1, c2, c3, c4 = st.columns(4)
                    c1.metric("è¶‹åŠ¿", trend)
                    c2.metric("RSI", f"{int(rsi_last) if rsi_last is not None else 'N/A'} ({rsi_tag})")
                    c3.metric("ADX", f"{adx_last:.1f} ({adx_tag})" if adx_last is not None else "N/A")
                    c4.metric("æ³¢åŠ¨ç‡(å¹´åŒ–)", f"{vol_ann:.2%} ({vol_tag})" if vol_ann is not None else "N/A")
                    
                    fig_k_current = plot_candlestick_with_indicators(df_slice, date_col=date_col, indicators=ind_slice, time_span=sel_lbl, show_text=False)
                    st.plotly_chart(fig_k_current, width="stretch")
                    
                    try:
                        adx_series = adx_prefetch if 'adx_prefetch' in locals() and adx_prefetch is not None else compute_adx(df_symbol)
                        fig_adx = px.line(x=adx_series.index, y=adx_series.values, labels={"x": date_col, "y": "ADX"})
                        st.plotly_chart(fig_adx, width="stretch")
                        mod_sum = f"è¯åˆ¸: {name}({symbol}), è¡Œä¸š: {industry}, åˆ†æåŒºé—´: {sel_lbl}, æœ€æ–°æ”¶ç›˜ä»·: {df_symbol['close'].iloc[-1]:.2f}"
                        analysis_trend = explain_trend_adx(df_slice, adx_series, ma_short, ma_long)
                        render_analysis_card(analysis_trend)
                        try:
                            vol_series = df_slice["close"].pct_change(fill_method=None).rolling(20).std() * np.sqrt(252)
                        except Exception:
                            vol_series = pd.Series(dtype=float)
                        analysis_vol = explain_volatility_risk(vol_series)
                        render_analysis_card(analysis_vol)
                        advisor_k = get_or_generate_advisor_wrapper("Kçº¿ä¸æŒ‡æ ‡", 
                            {"time_range": sel_lbl, "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}, 
                            facts, sel_lbl, symbol, industry)
                        render_advisor_text(advisor_k)
                        render_followup("Kçº¿ä¸æŒ‡æ ‡", 
                            {"time_range": sel_lbl, "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}, 
                            advisor_k, facts)
                        st.subheader("3åˆ†é’Ÿä¿¡ä»»")
                        trust_cols = st.columns(4)
                        try:
                            from src.data_io import validate_tushare_token
                            token_ok = validate_tushare_token()
                        except Exception:
                            token_ok = False
                        trust_cols[0].metric("æ•°æ®æº", "Baostock/Tushare")
                        trust_cols[1].metric("Token", "æœ‰æ•ˆ" if token_ok else "æœªé…ç½®")
                        try:
                            prob_payload = load_module_result(ROOT, symbol, "æ¶¨è·Œæ¦‚ç‡åˆ†æ")
                            auc_val = float(prob_payload.get("data", {}).get("auc")) if prob_payload else None
                        except Exception:
                            auc_val = None
                        trust_cols[2].metric("æ¨¡å‹AUC", f"{auc_val:.2f}" if auc_val is not None else "N/A")
                        trust_cols[3].metric("åŒºé—´", sel_lbl)
                    except Exception:
                        st.info("ADXè®¡ç®—å¤±è´¥")
                
                elif module_sel == "ç›¸å…³æ€§åˆ†æ":
                    st.subheader("ç›¸å…³æ€§åˆ†æ")
                    view_lbl = st.session_state.get("view_range_label", "ä¸€å¹´")
                    rng_days = {"ä¸€å‘¨": 7, "ä¸€ä¸ªæœˆ": 30, "ä¸‰ä¸ªæœˆ": 90, "å…­ä¸ªæœˆ": 180, "ä¸€å¹´": 365, "ä¸‰å¹´": 365*3, "äº”å¹´": 365*5}
                    df_s = df_symbol[[date_col, "close"]].copy()
                    df_s[date_col] = pd.to_datetime(df_s[date_col])
                    df_s = df_s.sort_values(by=date_col)
                    r_raw_s = df_s["close"].divide(df_s["close"].shift(1)) - 1
                    ret_s = pd.Series(r_raw_s.values, index=df_s[date_col]).dropna()
                    bench_codes = {"ä¸Šè¯æŒ‡æ•°": "sh.000001", "æ²ªæ·±300": "sh.000300", "åˆ›ä¸šæ¿æŒ‡": "sz.399006"}
                    bench_rets = {}
                    for nm, code in bench_codes.items():
                        try:
                            df_b = fetch_kline_baostock(code, str(sd_local), str(ed_local), frequency)
                            if not df_b.empty:
                                df_b = df_b.sort_values(by="date")
                                r_raw_b = df_b["close"].divide(df_b["close"].shift(1)) - 1
                                r_b = pd.Series(r_raw_b.values, index=df_b["date"]).dropna()
                                bench_rets[nm] = r_b
                        except Exception:
                            continue
                    pc = None
                    try:
                        from src.scheduler import build_pivot_close as _build_pc
                        df_map = st.session_state.get("stock_map")
                        if df_map is None or df_map.empty:
                            st.warning("è¡Œä¸šæ˜ å°„æœªåŠ è½½ï¼Œæ— æ³•æ„é€ è¡Œä¸šæŒ‡æ•°")
                        elif industry not in df_map["industry"].values:
                            fallback_ind = st.session_state.get("ind_sel_right")
                            if fallback_ind and fallback_ind in df_map["industry"].values:
                                st.info(f"å½“å‰è¡Œä¸šä¸åœ¨æ˜ å°„ä¸­ï¼Œä½¿ç”¨å³ä¾§é€‰æ‹©è¡Œä¸š: {fallback_ind}")
                                industry = fallback_ind
                            else:
                                st.warning(f"æ˜ å°„ä¸­ä¸åŒ…å«å½“å‰è¡Œä¸š: {industry}")
                        else:
                            syms = df_map[df_map["industry"] == industry]["symbol"].dropna().astype(str).tolist()
                            st.caption(f"è¡Œä¸šæˆåˆ†æ•°: {len(syms)}")
                            pc = _build_pc(st.session_state, industry, sd_local, ed_local, frequency)
                            if pc is not None:
                                st.caption(f"è¡Œä¸šé€è§†å½¢çŠ¶(å…¨éƒ¨åŒºé—´): {pc.shape}")
                    except Exception:
                        pc = None
                    if pc is not None and not pc.empty and view_lbl != "å…¨éƒ¨":
                        start_dt = pd.to_datetime(df_symbol[date_col]).max() - pd.Timedelta(days=rng_days.get(view_lbl, 365))
                        pc_full = pc
                        pc = pc.loc[pc.index >= start_dt]
                        if pc.empty:
                            pc = pc_full
                            st.info("æ‰€é€‰åŒºé—´å†…è¡Œä¸šæ•°æ®ä¸ºç©ºï¼Œå·²å›é€€åˆ°å…¨éƒ¨åŒºé—´")
                        else:
                            st.caption(f"è¡Œä¸šé€è§†å½¢çŠ¶(æ‰€é€‰åŒºé—´): {pc.shape}")
                    ind_ret = None
                    if pc is not None and not pc.empty:
                        X = pc.ffill()
                        X = X.divide(X.shift(1)) - 1
                        ind_ret = X.dropna().mean(axis=1)
                        ind_ret.name = "è¡Œä¸šæŒ‡æ•°"
                    else:
                        st.warning("æœªèƒ½æ„é€ è¡Œä¸šæŒ‡æ•°ï¼Œæ£€æŸ¥æ˜ å°„ä¸æœ¬åœ°æ•°æ®æˆ–ç½‘ç»œé‡‡é›†")
                    cross_assets = {}
                    try:
                        import yfinance as yf
                        ca_map = {"é»„é‡‘": "GC=F", "åŸæ²¹": "CL=F", "USDCNH": "USDCNH=X", "TLT": "TLT"}
                        for nm, tk in ca_map.items():
                            try:
                                df_ca = yf.download(tk, start=str(sd_local), end=str(ed_local), interval="1d", progress=False)
                                if not df_ca.empty and "Close" in df_ca.columns:
                                    df_ca.index = pd.to_datetime(df_ca.index)
                                    r = (df_ca["Close"].divide(df_ca["Close"].shift(1)) - 1).dropna()
                                    cross_assets[nm] = r
                            except Exception:
                                continue
                    except Exception:
                        pass
                    all_series = {"è¯åˆ¸": ret_s}
                    for k, v in bench_rets.items():
                        all_series[k] = v
                    if ind_ret is not None:
                        all_series["è¡Œä¸šæŒ‡æ•°"] = ind_ret
                    for k, v in cross_assets.items():
                        all_series[k] = v
                    series_norm = {}
                    for k, v in all_series.items():
                        if isinstance(v, pd.Series):
                            series_norm[k] = pd.Series(np.asarray(v.values).reshape(-1), index=v.index)
                        elif isinstance(v, pd.DataFrame) and v.shape[1] == 1:
                            col0 = v.columns[0]
                            series_norm[k] = pd.Series(np.asarray(v[col0].values).reshape(-1), index=v.index)
                        else:
                            try:
                                vv = pd.Series(v)
                                series_norm[k] = vv
                            except Exception:
                                pass
                    df_ret = pd.DataFrame(series_norm)
                    corr = df_ret.corr()
                    fig_corr = plot_corr_heatmap(corr)
                    st.plotly_chart(fig_corr, width="stretch")
                    def _lvl(r):
                        a = abs(r)
                        return "é«˜åº¦ç›¸å…³" if a >= 0.7 else ("ä¸­åº¦ç›¸å…³" if a >= 0.3 else "å¼±ç›¸å…³")
                    lines = []
                    if "è¯åˆ¸" in corr.index and "æ²ªæ·±300" in corr.index:
                        r_sym_hs = float(corr.loc["è¯åˆ¸","æ²ªæ·±300"])
                        if np.isfinite(r_sym_hs):
                            dir_txt = "æ­£ç›¸å…³" if r_sym_hs >= 0 else "è´Ÿç›¸å…³"
                            lines.append(f"æ ‡çš„è¯åˆ¸ä¸æ²ªæ·±300å‘ˆ{_lvl(r_sym_hs)}ï¼ˆ{r_sym_hs:.3f}ï¼‰{dir_txt}ï¼ŒÎ²æ•å£{'è¾ƒé«˜' if abs(r_sym_hs)>=0.4 else 'å¯æ§'}ã€‚")
                        else:
                            lines.append("æ ‡çš„è¯åˆ¸ä¸æ²ªæ·±300ç›¸å…³æ€§æ— æ³•è®¡ç®—ï¼Œå¯èƒ½ç”±äºæ ·æœ¬åŒºé—´å†…æœ‰æ•ˆæ•°æ®ä¸è¶³ã€‚")
                    if "è¯åˆ¸" in corr.index and "åˆ›ä¸šæ¿æŒ‡" in corr.index:
                        r_sym_cyb = float(corr.loc["è¯åˆ¸","åˆ›ä¸šæ¿æŒ‡"])
                        if np.isfinite(r_sym_cyb):
                            dir_txt = "æ­£ç›¸å…³" if r_sym_cyb >= 0 else "è´Ÿç›¸å…³"
                            lines.append(f"æ ‡çš„è¯åˆ¸ä¸åˆ›ä¸šæ¿æŒ‡æ•°ç›¸å…³æ€§ {r_sym_cyb:.3f}ï¼ˆ{_lvl(r_sym_cyb)}ï¼Œ{dir_txt}ï¼‰ã€‚")
                        else:
                            lines.append("æ ‡çš„è¯åˆ¸ä¸åˆ›ä¸šæ¿æŒ‡æ•°ç›¸å…³æ€§æ— æ³•è®¡ç®—ï¼Œå¯èƒ½ç”±äºæ ·æœ¬åŒºé—´å†…æœ‰æ•ˆæ•°æ®ä¸è¶³ã€‚")
                    if "è¯åˆ¸" in corr.index and "è¡Œä¸šæŒ‡æ•°" in corr.index:
                        r_sym_ind = float(corr.loc["è¯åˆ¸","è¡Œä¸šæŒ‡æ•°"])
                        if np.isfinite(r_sym_ind):
                            dir_txt = "æ­£ç›¸å…³" if r_sym_ind >= 0 else "è´Ÿç›¸å…³"
                            lines.append(f"æ ‡çš„è¯åˆ¸ä¸è¡Œä¸šæŒ‡æ•°ç›¸å…³æ€§ {r_sym_ind:.3f}ï¼ˆ{_lvl(r_sym_ind)}ï¼Œ{dir_txt}ï¼‰ï¼Œåæ˜ ä¸åŒä¸šè”åŠ¨ç¨‹åº¦ã€‚")
                        else:
                            lines.append("è¡Œä¸šæŒ‡æ•°ç›¸å…³æ€§æ— æ³•è®¡ç®—ï¼Œå¯èƒ½ç”±äºæ ·æœ¬åŒºé—´å†…æœ‰æ•ˆæ•°æ®ä¸è¶³ã€‚")
                    ca_list = ["é»„é‡‘","åŸæ²¹","TLT","USDCNH"]
                    low_assets = []
                    per_assets = []
                    for nm in ca_list:
                        if nm in corr.index and "è¯åˆ¸" in corr.index:
                            rv = float(corr.loc["è¯åˆ¸", nm])
                            if np.isfinite(rv):
                                if abs(rv) < 0.1:
                                    low_assets.append(nm)
                                else:
                                    dir_txt = "æ­£ç›¸å…³" if rv >= 0 else "è´Ÿç›¸å…³"
                                    per_assets.append(f"æ ‡çš„è¯åˆ¸ä¸{nm}{dir_txt}ï¼ˆ{rv:.3f}ï¼Œ{_lvl(rv)}ï¼‰")
                            else:
                                per_assets.append(f"æ ‡çš„è¯åˆ¸ä¸{nm}ç›¸å…³æ€§æ— æ³•è®¡ç®—ï¼Œå¯èƒ½ç”±äºæ ·æœ¬åŒºé—´å†…æœ‰æ•ˆæ•°æ®ä¸è¶³ã€‚")
                    if low_assets:
                        lines.append(f"æ ‡çš„è¯åˆ¸ä¸{', '.join(low_assets)}ç›¸å…³æ€§å‡ä½äº 0.1ï¼ˆè¿‘ä¼¼é›¶ï¼‰ï¼Œä¸é¿é™©/å¤§å®—èµ„äº§å…³è”åº¦ä¸é«˜ã€‚")
                    lines.extend(per_assets)
                    hs300 = bench_rets.get("æ²ªæ·±300")
                    anomaly_txt = None
                    if hs300 is not None and not hs300.empty:
                        comb_an = pd.DataFrame({"è¯åˆ¸": ret_s, "æ²ªæ·±300": hs300}).dropna()
                        if not comb_an.empty:
                            rc_full = float(comb_an["è¯åˆ¸"].corr(comb_an["æ²ªæ·±300"]))
                            rc_roll = float(comb_an["è¯åˆ¸"].rolling(window=60).corr(comb_an["æ²ªæ·±300"]).dropna().iloc[-1]) if comb_an.shape[0] >= 60 else rc_full
                            diff = rc_roll - rc_full
                            if abs(diff) >= 0.2 or (np.sign(rc_roll) != np.sign(rc_full)):
                                anomaly_txt = f"æ ‡çš„è¯åˆ¸ä¸æ²ªæ·±300è¿‘æœŸæ»šåŠ¨ç›¸å…³æ€§å˜åŒ–æ˜¾è‘—ï¼ˆå½“å‰ {rc_roll:.2f}ï¼Œå…¨æ ·æœ¬ {rc_full:.2f}ï¼‰ï¼Œéœ€å…³æ³¨å…³ç³»å˜åŒ–ã€‚"
                    if anomaly_txt:
                        lines.append(anomaly_txt)
                    st.subheader("ç›¸å…³æ€§åˆ†æç»“è®º")
                    for sline in lines:
                        st.markdown(f"- {sline}")
                    mod_sum = "ï¼›".join(lines) if lines else f"ç»¼åˆç›¸å…³æ€§: å¹³å‡ç›¸å…³æ€§: {corr.mean().mean():.3f}"
                    advisor_inputs = {"time_range": view_lbl, "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                    hs300 = bench_rets.get("æ²ªæ·±300")
                    if hs300 is not None and not hs300.empty:
                        comb = pd.DataFrame({"è¯åˆ¸": ret_s, "æ²ªæ·±300": hs300}).dropna()
                        fig_scatter = plot_returns_scatter(comb["æ²ªæ·±300"], comb["è¯åˆ¸"], "æ²ªæ·±300", "è¯åˆ¸")
                        st.plotly_chart(fig_scatter, width="stretch")
                        if comb.shape[0] >= 10:
                            r_scatter = float(comb["è¯åˆ¸"].corr(comb["æ²ªæ·±300"]))
                            if np.isfinite(r_scatter):
                                lvl_txt = "é«˜åº¦" if abs(r_scatter) >= 0.7 else ("ä¸­åº¦" if abs(r_scatter) >= 0.3 else "å¼±")
                                dir_txt = "æ­£ç›¸å…³" if r_scatter >= 0 else "è´Ÿç›¸å…³"
                                tail_txt = "çŸ­æœŸæ”¶ç›Šå˜åŒ–è¶‹åŠ¿åŸºæœ¬ä¸€è‡´" if r_scatter >= 0 else "çŸ­æœŸæ”¶ç›Šå˜åŒ–è¶‹åŠ¿ç›¸å"
                                st.markdown(f"- æ ‡çš„è¯åˆ¸ä¸æ²ªæ·±300æ”¶ç›Šç‡æ•£ç‚¹å›¾æ˜¾ç¤ºå‡º{lvl_txt}{dir_txt}ï¼ˆPearson r={r_scatter:.3f}ï¼‰ï¼Œ{tail_txt}ã€‚")
                            else:
                                st.markdown("- æ”¶ç›Šç‡æ•£ç‚¹å›¾æ•°æ®ä¸è¶³æˆ–ç¼ºå¤±ã€‚")
                        else:
                            st.markdown("- æ”¶ç›Šç‡æ•£ç‚¹å›¾æ•°æ®ä¸è¶³æˆ–ç¼ºå¤±ã€‚")
                        roll = comb["è¯åˆ¸"].rolling(window=60).corr(comb["æ²ªæ·±300"])
                        fig_roll = plot_rolling_corr(roll, "60æ—¥æ»šåŠ¨ç›¸å…³ç³»æ•°")
                        st.plotly_chart(fig_roll, width="stretch")
                        roll_clean = roll.dropna()
                        if not roll_clean.empty:
                            last_r = float(roll_clean.iloc[-1])
                            mean_r = float(roll_clean.mean())
                            std_r = float(roll_clean.std())
                            if np.isfinite(last_r) and np.isfinite(mean_r):
                                overall = "é«˜åº¦æ­£ç›¸å…³" if mean_r >= 0.7 else ("ä¸­åº¦æ­£ç›¸å…³" if 0.3 <= mean_r < 0.7 else ("è´Ÿç›¸å…³" if mean_r < 0 else "ç›¸å…³æ€§å¼±"))
                                trend_delta = last_r - mean_r
                                trend_txt = "è¿‘æœŸç›¸å…³æ€§ä¸Šå‡" if trend_delta > 0.1 else ("è¿‘æœŸç›¸å…³æ€§ä¸‹é™" if trend_delta < -0.1 else "è¿‘æœŸç›¸å…³æ€§å˜åŒ–ä¸æ˜æ˜¾")
                                stab_txt = "ç›¸å…³æ€§ä¸ç¨³å®šï¼Œæ³¢åŠ¨è¾ƒå¤§" if std_r > 0.3 else "ç›¸å…³æ€§è¾ƒä¸ºç¨³å®š"
                                st.markdown(f"- æ ‡çš„è¯åˆ¸ä¸æ²ªæ·±30060æ—¥æ»šåŠ¨ç›¸å…³ç³»æ•°æ•´ä½“å‘ˆ{overall}ï¼ˆå‡å€¼={mean_r:.2f}ï¼‰ï¼Œ{trend_txt}ï¼Œ{stab_txt}ã€‚")
                            else:
                                st.markdown("- 60æ—¥æ»šåŠ¨ç›¸å…³ç³»æ•°æ•°æ®ä¸è¶³æˆ–ç¼ºå¤±ã€‚")
                        else:
                            st.markdown("- 60æ—¥æ»šåŠ¨ç›¸å…³ç³»æ•°æ•°æ®ä¸è¶³æˆ–ç¼ºå¤±ã€‚")
                        cov = float(comb["è¯åˆ¸"].cov(comb["æ²ªæ·±300"]))
                        var_b = float(comb["æ²ªæ·±300"].var())
                        beta = cov / var_b if var_b != 0 else np.nan
                        st.metric("Î²ç³»æ•°(ç›¸å¯¹æ²ªæ·±300)", f"{beta:.2f}" if np.isfinite(beta) else "N/A")
                    render_analysis_card(explain_correlation_risk(corr))
                    advisor_c = get_or_generate_advisor_wrapper("ç›¸å…³æ€§åˆ†æ", advisor_inputs, facts, view_lbl, symbol, industry)
                    render_advisor_text(advisor_c)
                    render_followup("ç›¸å…³æ€§åˆ†æ", advisor_inputs, advisor_c, facts)
                    mod_cache["ç›¸å…³æ€§åˆ†æ"] = {"corr": corr, "fig_corr": fig_corr, "advisor_inputs": advisor_inputs}
                
                elif module_sel == "PCAåˆ†æ":
                    if "PCAåˆ†æ" not in mod_cache:
                        payload = load_module_result(ROOT, symbol, "PCAåˆ†æ")
                        if payload and payload.get("data", {}).get("explained"):
                            explained = pd.Series(payload["data"]["explained"])
                            fig_pca = plot_pca_explained(explained)
                            mod_sum = f"PCAåˆ†æ: å‰3ä¸ªä¸»æˆåˆ†ç´¯è®¡è§£é‡Šæ–¹å·®: {sum(explained[:3]):.2%}"
                            advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            mod_cache["PCAåˆ†æ"] = {"explained": explained, "fig_pca": fig_pca, "advisor_inputs": advisor_inputs}
                    st.subheader("PCAåˆ†æ")
                    cached = mod_cache.get("PCAåˆ†æ")
                    if cached:
                        st.plotly_chart(cached["fig_pca"], width="stretch")
                        render_analysis_card(explain_pca_structure(cached["explained"]))
                        advisor_p = get_or_generate_advisor_wrapper("PCAåˆ†æ", cached["advisor_inputs"], facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                        render_advisor_text(advisor_p)
                        render_followup("PCAåˆ†æ", cached["advisor_inputs"], advisor_p, facts)
                    elif "stock_map" in st.session_state and st.session_state["stock_map"] is not None:
                        df_map = st.session_state["stock_map"]
                        if industry and industry in df_map["industry"].values:
                            df_ind = df_map[df_map["industry"] == industry]
                            symbols_in_industry = df_ind["symbol"].dropna().astype(str).tolist()
                            
                            # è¯»å–è¡Œä¸šå†…å…¶ä»–è‚¡ç¥¨æ•°æ®
                            all_dfs = []
                            for sym in symbols_in_industry:
                                try:
                                    sym_df = fetch_resolved_df(st.session_state, sym, industry, str(sd_local), str(ed_local), frequency)
                                    if not sym_df.empty:
                                        sym_df = sym_df[['date', 'close']].rename(columns={'close': sym})
                                        sym_df['date'] = pd.to_datetime(sym_df['date'])
                                        all_dfs.append(sym_df)
                                except:
                                    continue
                            
                            if all_dfs:
                                # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
                                from functools import reduce
                                pivot_close = reduce(lambda left, right: pd.merge(left, right, on='date', how='outer'), all_dfs)
                                pivot_close = pivot_close.set_index('date').sort_index()
                                
                                # è®¡ç®—PCA
                                X = pivot_close.pct_change(fill_method=None).dropna()
                                if not X.empty:
                                    pca_model, explained = compute_pca(X)
                                    fig_pca = plot_pca_explained(explained)
                                    st.plotly_chart(fig_pca, width="stretch")
                                    render_analysis_card(explain_pca_structure(explained))
                                    # å¤§æ¨¡å‹åˆ†æ
                                    mod_sum = f"PCAåˆ†æ: å‰3ä¸ªä¸»æˆåˆ†ç´¯è®¡è§£é‡Šæ–¹å·®: {sum(explained[:3]):.2%}"
                                    advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                                    advisor_p = get_or_generate_advisor_wrapper("PCAåˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                                    render_advisor_text(advisor_p)
                                    render_followup("PCAåˆ†æ", advisor_inputs, advisor_p, facts)
                                    mod_cache["PCAåˆ†æ"] = {"explained": explained, "fig_pca": fig_pca, "advisor_inputs": advisor_inputs}
                                else:
                                    st.warning("æ•°æ®ä¸è¶³è¿›è¡ŒPCAåˆ†æ")
                            else:
                                st.warning("æ— æ³•è·å–è¡Œä¸šå†…å…¶ä»–è‚¡ç¥¨æ•°æ®")
                        else:
                            st.warning("æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨å¯¹åº”çš„è¡Œä¸šä¿¡æ¯")
                    else:
                        st.warning("è¯·å…ˆä¸Šä¼ è¡Œä¸šæ˜ å°„æ–‡ä»¶")
                
                elif module_sel == "æ³¢åŠ¨æ€§åˆ†æ":
                    if "æ³¢åŠ¨æ€§åˆ†æ" not in mod_cache:
                        payload = load_module_result(ROOT, symbol, "æ³¢åŠ¨æ€§åˆ†æ")
                        if payload and payload.get("data", {}).get("vol_values") is not None:
                            try:
                                idx_try = pd.to_datetime(payload["data"]["vol_index"], errors="coerce")
                                if hasattr(idx_try, "isna") and idx_try.isna().any():
                                    tail_idx = pd.to_datetime(df_symbol[date_col]).iloc[-len(payload["data"]["vol_values"]):]
                                    idx = tail_idx
                                else:
                                    idx = idx_try
                            except Exception:
                                idx = pd.to_datetime(df_symbol[date_col]).iloc[-len(payload["data"]["vol_values"]):]
                            n = min(len(payload["data"]["vol_values"]), len(idx))
                            vol = pd.Series(list(payload["data"]["vol_values"])[-n:], index=idx[-n:])
                            fig_vol = px.line(x=vol.index, y=vol.values, labels={"x": date_col, "y": "HV(20)"})
                            sigma2 = payload["data"].get("sigma2")
                            mod_sum = f"æ³¢åŠ¨æ€§åˆ†æ: HV20={vol.iloc[-1]:.4f}, GARCHé¢„æµ‹æ–¹å·®={sigma2 if sigma2 else 'N/A'}"
                            advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            mod_cache["æ³¢åŠ¨æ€§åˆ†æ"] = {"vol": vol, "fig_vol": fig_vol, "sigma2": sigma2, "advisor_inputs": advisor_inputs}
                    cached = mod_cache.get("æ³¢åŠ¨æ€§åˆ†æ")
                    if cached:
                        vol = cached["vol"]; fig_vol = cached["fig_vol"]; sigma2 = cached["sigma2"]; advisor_inputs = cached["advisor_inputs"]
                        st.plotly_chart(fig_vol, width="stretch")
                        if sigma2 is not None:
                            st.metric(label="GARCHé¢„æµ‹æ–¹å·®(ä¸‹ä¸€æœŸ)", value=f"{sigma2:.6f}")
                        render_analysis_card(explain_volatility_risk(vol, sigma2))
                        advisor_v = get_or_generate_advisor_wrapper("æ³¢åŠ¨æ€§åˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                        render_advisor_text(advisor_v)
                        render_followup("æ³¢åŠ¨æ€§åˆ†æ", advisor_inputs, advisor_v, facts)
                    else:
                        try:
                            vol = compute_volatility(df_symbol["close"], window=20)
                            fig_vol = px.line(x=vol.index, y=vol.values, labels={"x": date_col, "y": "HV(20)"})
                            st.plotly_chart(fig_vol, width="stretch")
                            
                            sigma2 = None
                            try:
                                res, forecast = compute_garch(df_symbol["close"])
                                sigma2 = forecast.variance.values[-1][-1] if forecast is not None else None
                                if sigma2:
                                    st.metric(label="GARCHé¢„æµ‹æ–¹å·®(ä¸‹ä¸€æœŸ)", value=f"{sigma2:.6f}")
                            except Exception:
                                pass
                            render_analysis_card(explain_volatility_risk(vol, sigma2))
                            mod_sum = f"æ³¢åŠ¨æ€§åˆ†æ: HV20={vol.iloc[-1]:.4f}, GARCHé¢„æµ‹æ–¹å·®={sigma2 if sigma2 else 'N/A'}"
                            advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            advisor_v = get_or_generate_advisor_wrapper("æ³¢åŠ¨æ€§åˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                            render_advisor_text(advisor_v)
                            render_followup("æ³¢åŠ¨æ€§åˆ†æ", advisor_inputs, advisor_v, facts)
                            mod_cache["æ³¢åŠ¨æ€§åˆ†æ"] = {"vol": vol, "fig_vol": fig_vol, "sigma2": sigma2, "advisor_inputs": advisor_inputs}
                        except Exception as e:
                            st.info(f"æ³¢åŠ¨æ€§åˆ†æå¤±è´¥: {str(e)}")
                
                elif module_sel == "å­£èŠ‚æ€§åˆ†æ":
                    if "å­£èŠ‚æ€§åˆ†æ" not in mod_cache:
                        payload = load_module_result(ROOT, symbol, "å­£èŠ‚æ€§åˆ†æ")
                        d = payload["data"] if payload else {}
                        if d.get("trend_values") and d.get("seasonal_values") and d.get("resid_values"):
                            trend = pd.Series(d["trend_values"], index=pd.to_datetime(d["trend_index"]))
                            seasonal = pd.Series(d["seasonal_values"], index=pd.to_datetime(d["seasonal_index"]))
                            resid = pd.Series(d["resid_values"], index=pd.to_datetime(d["resid_index"]))
                            fig_stl = plot_stl_components(trend, seasonal, resid)
                            fig_ap = None
                            if d.get("acf") and d.get("pacf"):
                                a_vals = pd.Series(d["acf"])
                                p_vals = pd.Series(d["pacf"])
                                fig_ap = plot_acf_pacf(a_vals, p_vals)
                            mod_sum = "å­£èŠ‚æ€§åˆ†è§£åˆ†æ: å±•ç¤ºäº†è¶‹åŠ¿ã€å­£èŠ‚æ€§å’Œæ®‹å·®åˆ†é‡"
                            advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            mod_cache["å­£èŠ‚æ€§åˆ†æ"] = {"stl_res": type("STL", (), {"trend": trend, "seasonal": seasonal, "resid": resid})(), "fig_stl": fig_stl, "fig_ap": fig_ap, "advisor_inputs": advisor_inputs}
                    cached = mod_cache.get("å­£èŠ‚æ€§åˆ†æ")
                    if cached:
                        stl_res = cached["stl_res"]; fig_stl = cached["fig_stl"]; fig_ap = cached.get("fig_ap"); advisor_inputs = cached["advisor_inputs"]
                        if fig_stl is not None: st.plotly_chart(fig_stl, width="stretch")
                        if fig_ap is not None: st.plotly_chart(fig_ap, width="stretch")
                        render_analysis_card(explain_seasonality(stl_res))
                        advisor_s = get_or_generate_advisor_wrapper("å­£èŠ‚æ€§åˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                        render_advisor_text(advisor_s)
                        render_followup("å­£èŠ‚æ€§åˆ†æ", advisor_inputs, advisor_s, facts)
                    else:
                        try:
                            stl_res = compute_stl(df_symbol.set_index(date_col)["close"], period=7)
                            fig_stl = plot_stl_components(stl_res.trend, stl_res.seasonal, stl_res.resid)
                            st.plotly_chart(fig_stl, width="stretch")
                            
                            fig_ap = None
                            try:
                                a_vals, p_vals = compute_acf_pacf(df_symbol["close"].pct_change().dropna())
                                fig_ap = plot_acf_pacf(a_vals, p_vals)
                                st.plotly_chart(fig_ap, width="stretch")
                            except Exception:
                                pass
                            render_analysis_card(explain_seasonality(stl_res))
                            mod_sum = "å­£èŠ‚æ€§åˆ†è§£åˆ†æ: å±•ç¤ºäº†è¶‹åŠ¿ã€å­£èŠ‚æ€§å’Œæ®‹å·®åˆ†é‡"
                            advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            advisor_s = get_or_generate_advisor_wrapper("å­£èŠ‚æ€§åˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                            render_advisor_text(advisor_s)
                            render_followup("å­£èŠ‚æ€§åˆ†æ", advisor_inputs, advisor_s, facts)
                            mod_cache["å­£èŠ‚æ€§åˆ†æ"] = {"stl_res": stl_res, "fig_stl": fig_stl, "fig_ap": fig_ap, "advisor_inputs": advisor_inputs}
                        except Exception as e:
                            st.info(f"å­£èŠ‚æ€§åˆ†æå¤±è´¥: {str(e)}")
                
                elif module_sel == "é£é™©-æ”¶ç›Šèšç±»åˆ†æ":
                    if "é£é™©-æ”¶ç›Šèšç±»åˆ†æ" not in mod_cache:
                        payload = load_module_result(ROOT, symbol, "é£é™©-æ”¶ç›Šèšç±»åˆ†æ")
                        d = payload["data"] if payload else {}
                        if d.get("ret") and d.get("vol") and d.get("labels") and d.get("index"):
                            feat = pd.DataFrame({"ret": pd.Series(d["ret"], index=d["index"]), "vol": pd.Series(d["vol"], index=d["index"])})
                            labels = pd.Series(d["labels"], index=d["index"])
                            fig_cluster = plot_cluster_scatter(feat, labels)
                            mod_sum = f"èšç±»åˆ†æ: å°†{feat.shape[0]}åªè‚¡ç¥¨åˆ†ä¸º{int(d.get('n_clusters', 0))}ç±»"
                            advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            mod_cache["é£é™©-æ”¶ç›Šèšç±»åˆ†æ"] = {"fig_cluster": fig_cluster, "labels": labels, "n_clusters": int(d.get("n_clusters", 0)), "advisor_inputs": advisor_inputs}
                    st.subheader("é£é™©-æ”¶ç›Šèšç±»åˆ†æ")
                    cached = mod_cache.get("é£é™©-æ”¶ç›Šèšç±»åˆ†æ")
                    if cached:
                        st.plotly_chart(cached["fig_cluster"], width="stretch")
                        render_analysis_card(explain_clustering(cached["n_clusters"], cached["labels"]))
                        advisor_km = get_or_generate_advisor_wrapper("é£é™©-æ”¶ç›Šèšç±»åˆ†æ", cached["advisor_inputs"], facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                        render_advisor_text(advisor_km)
                        render_followup("é£é™©-æ”¶ç›Šèšç±»åˆ†æ", cached["advisor_inputs"], advisor_km, facts)
                    elif "stock_map" in st.session_state and st.session_state["stock_map"] is not None:
                        df_map = st.session_state["stock_map"]
                        if industry and industry in df_map["industry"].values:
                            df_ind = df_map[df_map["industry"] == industry]
                            symbols_in_industry = df_ind["symbol"].dropna().astype(str).tolist()
                            
                            # è¯»å–è¡Œä¸šå†…å…¶ä»–è‚¡ç¥¨æ•°æ®
                            all_dfs = []
                            for sym in symbols_in_industry:
                                try:
                                    sym_df = fetch_resolved_df(st.session_state, sym, industry, str(sd_local), str(ed_local), frequency)
                                    if not sym_df.empty:
                                        sym_df = sym_df[['date', 'close']].rename(columns={'close': sym})
                                        sym_df['date'] = pd.to_datetime(sym_df['date'])
                                        all_dfs.append(sym_df)
                                except:
                                    continue
                            
                            if all_dfs:
                                # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
                                from functools import reduce
                                pivot_close = reduce(lambda left, right: pd.merge(left, right, on='date', how='outer'), all_dfs)
                                pivot_close = pivot_close.set_index('date').sort_index()
                                
                                # è®¡ç®—æ”¶ç›Šç‡å’Œæ³¢åŠ¨ç‡
                                rets = pivot_close.pct_change(fill_method=None).dropna()
                                if not rets.empty and rets.shape[1] >= 3:
                                    feat = pd.DataFrame({
                                        "ret": rets.mean() * 252,
                                        "vol": rets.std() * np.sqrt(252)
                                    })
                                    feat = feat.dropna()
                                    
                                    if feat.shape[0] >= 3:
                                        n_clusters = min(3, max(2, feat.shape[0]//2))
                                        km, labels = compute_kmeans(feat, n_clusters=n_clusters)
                                        fig_cluster = plot_cluster_scatter(feat, labels)
                                        st.plotly_chart(fig_cluster, width="stretch")
                                        render_analysis_card(explain_clustering(n_clusters, labels))
                                        # å¤§æ¨¡å‹åˆ†æ
                                        mod_sum = f"èšç±»åˆ†æ: å°†{feat.shape[0]}åªè‚¡ç¥¨åˆ†ä¸º{n_clusters}ç±»"
                                        advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                                        advisor_km = get_or_generate_advisor_wrapper("é£é™©-æ”¶ç›Šèšç±»åˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                                        render_advisor_text(advisor_km)
                                        render_followup("é£é™©-æ”¶ç›Šèšç±»åˆ†æ", advisor_inputs, advisor_km, facts)
                                        mod_cache["é£é™©-æ”¶ç›Šèšç±»åˆ†æ"] = {"fig_cluster": fig_cluster, "labels": labels, "n_clusters": n_clusters, "advisor_inputs": advisor_inputs}
                                    else:
                                        st.warning("æ•°æ®ä¸è¶³è¿›è¡Œèšç±»åˆ†æ")
                                else:
                                    st.warning("æ•°æ®ä¸è¶³è¿›è¡Œèšç±»åˆ†æ")
                            else:
                                st.warning("æ— æ³•è·å–è¡Œä¸šå†…å…¶ä»–è‚¡ç¥¨æ•°æ®")
                        else:
                            st.warning("æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨å¯¹åº”çš„è¡Œä¸šä¿¡æ¯")
                    else:
                        st.warning("è¯·å…ˆä¸Šä¼ è¡Œä¸šæ˜ å°„æ–‡ä»¶")
                
                elif module_sel == "åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ":
                    if "åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ" not in mod_cache:
                        payload = load_module_result(ROOT, symbol, "åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ")
                        d = payload["data"] if payload else {}
                        rows = d.get("portrait")
                        if rows:
                            portrait = pd.DataFrame(rows)
                            fig_portrait = plot_factor_portrait(portrait.fillna(0.5))
                            mod_sum = f"å› å­ç”»åƒåˆ†æ: åˆ†æäº†{portrait.shape[0]}ä¸ªåŸºæœ¬é¢æŒ‡æ ‡"
                            advisor_inputs = {"analysis_mode": "portrait", "time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            mod_cache["åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ"] = {"fig_portrait": fig_portrait, "portrait": portrait.fillna(0.5), "advisor_inputs": advisor_inputs}
                    st.subheader("åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ")
                    funda_dir_default = str((ROOT / "data" / "fundamentals").resolve())
                    funda_dir = Path(st.session_state.get("funda_dir", funda_dir_default))
                    
                    cached = mod_cache.get("åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ")
                    if cached:
                        st.plotly_chart(cached["fig_portrait"], width="stretch")
                        render_analysis_card(explain_factor_portrait(cached["portrait"]))
                        advisor_fp = get_or_generate_advisor_wrapper("åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ", cached["advisor_inputs"], facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                        render_advisor_text(advisor_fp)
                        render_followup("åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ", cached["advisor_inputs"], advisor_fp, facts)
                    else:
                        try:
                            sym_dir = funda_dir / symbol.replace('.', '_')
                            if sym_dir.exists():
                                fi_fp = sym_dir / "fina_indicator.csv"
                                portrait_rows = []
                                if fi_fp.exists():
                                    fi = pd.read_csv(fi_fp)
                                    fi = fi.sort_values(["end_date","ann_date"]) if "end_date" in fi.columns and "ann_date" in fi.columns else fi
                                    metrics_cols = [c for c in ["roe","roa","grossprofit_margin","debt_to_assets","oper_cash_flow","pe","pb"] if c in fi.columns]
                                    if metrics_cols:
                                        for m in metrics_cols:
                                            ser = pd.to_numeric(fi[m], errors="coerce").dropna()
                                            val = float(ser.iloc[-1]) if not ser.empty else None
                                            trend = float((ser.diff().dropna().iloc[-1])) if ser.shape[0] >= 2 else 0.0
                                            portrait_rows.append({"metric": m, "value": val, "trend": trend})
                                if portrait_rows:
                                    portrait = pd.DataFrame(portrait_rows)
                                    fig_portrait = plot_factor_portrait(portrait.fillna(0.5))
                                    st.plotly_chart(fig_portrait, width="stretch")
                                    render_analysis_card(explain_factor_portrait(portrait.fillna(0.5)))
                                    mod_sum = f"å› å­ç”»åƒåˆ†æ: åˆ†æäº†{len(portrait_rows)}ä¸ªåŸºæœ¬é¢æŒ‡æ ‡"
                                    advisor_inputs = {"analysis_mode": "portrait", "time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                                    advisor_fp = get_or_generate_advisor_wrapper("åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                                    render_advisor_text(advisor_fp)
                                    render_followup("åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ", advisor_inputs, advisor_fp, facts)
                                    mod_cache["åŸºæœ¬é¢å› å­æš´éœ²åˆ†æ"] = {"fig_portrait": fig_portrait, "portrait": portrait.fillna(0.5), "advisor_inputs": advisor_inputs}
                                else:
                                    st.warning("æœªæ‰¾åˆ°åŸºæœ¬é¢æ•°æ®")
                            else:
                                st.warning("æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨å¯¹åº”çš„åŸºæœ¬é¢æ•°æ®ç›®å½•")
                                st.warning("è¯·å…ˆé‡‡é›†è¯¥è‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®")
                        except Exception as e:
                            st.info(f"åŸºæœ¬é¢å› å­åˆ†æå¤±è´¥: {str(e)}")
                
                elif module_sel == "æ¶¨è·Œæ¦‚ç‡åˆ†æ":
                    if "æ¶¨è·Œæ¦‚ç‡åˆ†æ" not in mod_cache:
                        payload = load_module_result(ROOT, symbol, "æ¶¨è·Œæ¦‚ç‡åˆ†æ")
                        d = payload["data"] if payload else {}
                        if d.get("proba") is not None and d.get("auc") is not None:
                            proba = pd.Series(d["proba"])
                            auc = float(d["auc"])
                            fig_prob = plot_probability_hist(proba)
                            mod_sum = f"æ¶¨è·Œæ¦‚ç‡é¢„æµ‹: AUC={auc:.3f}, å¹³å‡ä¸Šæ¶¨æ¦‚ç‡={proba.mean():.3f}"
                            advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                            mod_cache["æ¶¨è·Œæ¦‚ç‡åˆ†æ"] = {"proba": proba, "auc": auc, "fig_prob": fig_prob, "advisor_inputs": advisor_inputs}
                    st.subheader("æ¶¨è·Œæ¦‚ç‡åˆ†æ")
                    cached = mod_cache.get("æ¶¨è·Œæ¦‚ç‡åˆ†æ")
                    if cached:
                        if cached["fig_prob"] is not None: st.plotly_chart(cached["fig_prob"], width="stretch")
                        if cached["auc"] is not None: st.metric(label="AUC", value=f"{cached['auc']:.3f}")
                        render_analysis_card(explain_prediction_probability(cached["proba"], cached["auc"]))
                        advisor_lr = get_or_generate_advisor_wrapper("æ¶¨è·Œæ¦‚ç‡åˆ†æ", cached["advisor_inputs"], facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                        render_advisor_text(advisor_lr)
                        render_followup("æ¶¨è·Œæ¦‚ç‡åˆ†æ", cached["advisor_inputs"], advisor_lr, facts)
                    elif "stock_map" in st.session_state and st.session_state["stock_map"] is not None:
                        df_map = st.session_state["stock_map"]
                        if industry and industry in df_map["industry"].values:
                            df_ind = df_map[df_map["industry"] == industry]
                            symbols_in_industry = df_ind["symbol"].dropna().astype(str).tolist()
                            
                            # è¯»å–è¡Œä¸šå†…å…¶ä»–è‚¡ç¥¨æ•°æ®
                            all_dfs = []
                            for sym in symbols_in_industry:
                                try:
                                    sym_df = fetch_resolved_df(st.session_state, sym, industry, str(sd_local), str(ed_local), frequency)
                                    if not sym_df.empty:
                                        sym_df = sym_df[['date', 'close']].rename(columns={'close': sym})
                                        sym_df['date'] = pd.to_datetime(sym_df['date'])
                                        all_dfs.append(sym_df)
                                except:
                                    continue
                            
                            if all_dfs and len(all_dfs) >= 5:
                                # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
                                from functools import reduce
                                pivot_close = reduce(lambda left, right: pd.merge(left, right, on='date', how='outer'), all_dfs)
                                pivot_close = pivot_close.set_index('date').sort_index()
                                
                                # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
                                R = pivot_close.pct_change(fill_method=None).dropna().mean() * 252
                                thr = float(R.median())
                                y_cls = (R > thr).astype(int)
                                
                                # æ„å»ºç‰¹å¾
                                X_rows = []
                                for s in symbols_in_industry:
                                    try:
                                        sym_df = fetch_resolved_df(st.session_state, s, industry, str(sd_local), str(ed_local), frequency)
                                        if not sym_df.empty:
                                            sym_df['date'] = pd.to_datetime(sym_df['date'])
                                            sym_df = sym_df.sort_values('date')
                                            slope_sma = float(sym_df["close"].rolling(20).mean().diff().dropna().iloc[-1]) if "close" in sym_df.columns else None
                                            slope_ema = float(sym_df["close"].ewm(span=60, adjust=False).mean().diff().dropna().iloc[-1]) if "close" in sym_df.columns else None
                                            X_rows.append({"symbol": s, "sma_slope": slope_sma, "ema_slope": slope_ema})
                                    except:
                                        continue
                                
                                X = pd.DataFrame(X_rows).set_index("symbol")
                                X = X.loc[y_cls.index.intersection(X.index)]
                                
                                if not X.empty and X.shape[0] >= 5:
                                    model, proba, auc = compute_logistic_proba(X, y_cls.loc[X.index])
                                    fig_prob = plot_probability_hist(proba)
                                    st.plotly_chart(fig_prob, width="stretch")
                                    st.metric(label="AUC", value=f"{auc:.3f}")
                                    render_analysis_card(explain_prediction_probability(proba, auc))
                                    # å¤§æ¨¡å‹åˆ†æ
                                    mod_sum = f"æ¶¨è·Œæ¦‚ç‡é¢„æµ‹: AUC={auc:.3f}, å¹³å‡ä¸Šæ¶¨æ¦‚ç‡={proba.mean():.3f}"
                                    advisor_inputs = {"time_range": "æœ€è¿‘ä¸€å¹´", "stock_name": name, "stock_code": symbol, "industry": industry, "module_data_summary": mod_sum}
                                    advisor_lr = get_or_generate_advisor_wrapper("æ¶¨è·Œæ¦‚ç‡åˆ†æ", advisor_inputs, facts, "æœ€è¿‘ä¸€å¹´", symbol, industry)
                                    render_advisor_text(advisor_lr)
                                    render_followup("æ¶¨è·Œæ¦‚ç‡åˆ†æ", advisor_inputs, advisor_lr, facts)
                                    mod_cache["æ¶¨è·Œæ¦‚ç‡åˆ†æ"] = {"proba": proba, "auc": auc, "fig_prob": fig_prob, "advisor_inputs": advisor_inputs}
                                else:
                                    st.warning("æ•°æ®ä¸è¶³è¿›è¡Œé€»è¾‘å›å½’åˆ†æ")
                            else:
                                st.warning("è¡Œä¸šå†…è‚¡ç¥¨æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦5åªè‚¡ç¥¨")
                        else:
                            st.warning("æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨å¯¹åº”çš„è¡Œä¸šä¿¡æ¯")
                    else:
                        st.warning("è¯·å…ˆä¸Šä¼ è¡Œä¸šæ˜ å°„æ–‡ä»¶")
    else:
        pass

# å³ä¾§æ¨¡å—ï¼šè¡Œä¸šä¸è‚¡ç¥¨é€‰æ‹©
with right_col:
    if main_function == "è¯åˆ¸å¤šå› å­é‡åŒ–è¯„åˆ†":
        st.header("è¯åˆ¸å¤šå› å­é‡åŒ–è¯„åˆ†")
        # è¡Œä¸šé€‰æ‹©
        industries_all = []
        if "stock_map" in st.session_state:
            industries_all = sorted(set(st.session_state["stock_map"]["industry"].dropna().tolist()))
        
        if industries_all:
            selected_industry = st.selectbox("é€‰æ‹©è¡Œä¸š", industries_all, key="ind_sel_right")
            
            # è¡Œä¸šå†…è‚¡ç¥¨é€‰æ‹©
            df_map = st.session_state.get("stock_map")
            if df_map is not None:
                df_ind = df_map[df_map["industry"] == selected_industry]
                syms_ind = df_ind["symbol"].dropna().astype(str).tolist() if "symbol" in df_ind.columns else []
                names_ind = df_ind["name"].astype(str).tolist() if "name" in df_ind.columns else []
                
                if syms_ind:
                    # åˆ›å»ºè‚¡ç¥¨é€‰æ‹©ç•Œé¢
                    st.subheader("é€‰æ‹©è¡Œä¸šå†…è¯åˆ¸")
                    stock_options = {}
                    for i, (sym, name) in enumerate(zip(syms_ind, names_ind)):
                        if i < len(names_ind):
                            stock_options[f"{name}({sym})"] = sym
                        else:
                            stock_options[sym] = sym
                    
                    selected_labels = st.multiselect(
                        "é€‰æ‹©è¯åˆ¸",
                        options=list(stock_options.keys()),
                        default=list(stock_options.keys())[:min(3, len(stock_options))],
                        key="stock_sel_right"
                    )
                    
                    selected_symbols = [stock_options[label] for label in selected_labels]
                    st.session_state["selected_symbols"] = selected_symbols
                    
                    if selected_symbols:
                        # æƒé‡é…ç½®
                        st.subheader("æƒé‡é…ç½®")
                        with st.expander("æƒé‡é…ç½®è¯´æ˜", expanded=False):
                            st.caption("ç›ˆåˆ©èƒ½åŠ›æƒé‡ï¼šå…³æ³¨å…¬å¸èµšé’±æ•ˆç‡ä¸è´¨é‡ï¼ˆå¦‚ ROEã€æ¯›åˆ©ç‡ã€å‡€åˆ©ç‡ï¼‰ã€‚æƒé‡è¶Šé«˜ï¼Œè¶Šåå¥½ç¨³å¥ç›ˆåˆ©çš„å…¬å¸ã€‚")
                            st.caption("å¿å€ºèƒ½åŠ›æƒé‡ï¼šå…³æ³¨è´Ÿå€ºç»“æ„ä¸ç°é‡‘æµå¿ä»˜èƒ½åŠ›ï¼ˆå¦‚èµ„äº§è´Ÿå€ºç‡ã€æµåŠ¨/é€ŸåŠ¨æ¯”ç‡ã€ç»è¥ç°é‡‘æµï¼‰ã€‚æƒé‡è¶Šé«˜ï¼Œè¶Šé‡è§†æŠ—é£é™©èƒ½åŠ›ã€‚")
                            st.caption("æˆé•¿æ€§æƒé‡ï¼šå…³æ³¨è¥æ”¶ä¸åˆ©æ¶¦çš„å¢é•¿ï¼ˆå¦‚åŒæ¯”å¢é€Ÿã€å¤åˆå¢é€Ÿï¼‰ã€‚æƒé‡è¶Šé«˜ï¼Œè¶Šåå¥½å…·å¤‡æ‰©å¼ ä¸æˆé•¿æ½œåŠ›çš„å…¬å¸ã€‚")
                            st.caption("æŠ•èµ„å›æŠ¥æƒé‡ï¼šå…³æ³¨è‚¡ä¸œå›æŠ¥ä¸ä¼°å€¼æ€§ä»·æ¯”ï¼ˆå¦‚è‚¡æ¯ã€ROEâ‰ˆROIã€PE/PBåˆç†æ€§ï¼‰ã€‚æƒé‡è¶Šé«˜ï¼Œè¶Šåå¥½å½“å‰å›æŠ¥ä¸ä¼°å€¼æ›´å‹å¥½ã€‚")
                            st.caption("æç¤ºï¼šå››é¡¹æƒé‡ä¼šè‡ªåŠ¨å½’ä¸€åŒ–ï¼Œæ€»å’Œä¸º 1ï¼›å¯å…ˆé€‰é¢„è®¾ï¼Œå†ç”¨æ»‘å—å¾®è°ƒã€‚")
                        preset = st.radio("æƒé‡é¢„è®¾", ["ç¨³å¥","å‡è¡¡","è¿›å–"], index=1, horizontal=True, key="w_preset")
                        preset_vals = {"ç¨³å¥": (0.5,0.3,0.1,0.1), "å‡è¡¡": (0.4,0.3,0.2,0.1), "è¿›å–": (0.3,0.2,0.3,0.2)}
                        
                        if st.session_state.get("last_preset") != preset:
                            wp, ws, wg, wr = preset_vals[preset]
                            st.session_state["w_profit"] = wp
                            st.session_state["w_solv"] = ws
                            st.session_state["w_grow"] = wg
                            st.session_state["w_ret"] = wr
                            st.session_state["last_preset"] = preset
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            wp = st.slider("ç›ˆåˆ©èƒ½åŠ›æƒé‡", min_value=0.0, max_value=1.0, 
                                         value=float(st.session_state.get("w_profit",0.4)), step=0.05, key="w_profit_right")
                            ws = st.slider("å¿å€ºèƒ½åŠ›æƒé‡", min_value=0.0, max_value=1.0, 
                                         value=float(st.session_state.get("w_solv",0.3)), step=0.05, key="w_solv_right")
                        with col2:
                            wg = st.slider("æˆé•¿æ€§æƒé‡", min_value=0.0, max_value=1.0, 
                                         value=float(st.session_state.get("w_grow",0.2)), step=0.05, key="w_grow_right")
                            wr = st.slider("æŠ•èµ„å›æŠ¥æƒé‡", min_value=0.0, max_value=1.0, 
                                         value=float(st.session_state.get("w_ret",0.1)), step=0.05, key="w_ret_right")
                        
                        funda_dir_default = str((ROOT / "data" / "fundamentals").resolve())
                        funda_dir = Path(st.session_state.get("funda_dir", funda_dir_default))
                        missing = []
                        for s in selected_symbols:
                            sym_dir = funda_dir / s.replace('.','_')
                            need = []
                            for fn in ["fina_indicator.csv","balancesheet.csv","income.csv","cashflow.csv"]:
                                if not (sym_dir / fn).exists():
                                    need.append(fn)
                            if need:
                                missing.append((s, need))
                        if missing:
                            st.warning(f"æ‰€é€‰è¯åˆ¸ç¼ºå°‘è´¢æŠ¥æ•°æ®ï¼š{len(missing)} ä¸ªã€‚è¯„åˆ†ä¸ºé›¶æˆ–æŠ¥å‘Šæ•°æ®ç¼ºå¤±é€šå¸¸ç”±æ­¤å¯¼è‡´ã€‚")
                            with st.expander("ç¼ºå¤±è¯¦æƒ…", expanded=False):
                                for s, need in missing:
                                    st.markdown(f"- {s} ç¼ºå°‘: {', '.join(need)}")
                            colm1, colm2 = st.columns(2)
                            with colm1:
                                sd_fi = st.date_input("è´¢æŠ¥å¼€å§‹æ—¥æœŸ", value=datetime.now()-timedelta(days=365*3), key="fi_sd_right")
                            with colm2:
                                ed_fi = st.date_input("è´¢æŠ¥ç»“æŸæ—¥æœŸ", value=datetime.now(), key="fi_ed_right")
                            do_fetch = st.button("ä¸€é”®é‡‡é›†æ‰€é€‰è¯åˆ¸è´¢æŠ¥", key="fetch_fi_right")
                            if do_fetch:
                                ok = validate_tushare_token()
                                if not ok:
                                    st.error("Tushare Token æ— æ•ˆæˆ–æƒé™ä¸è¶³ï¼Œè¯·åœ¨ .env.local é…ç½® TUSHARE_TOKEN")
                                else:
                                    with st.spinner("æ­£åœ¨é‡‡é›†è´¢æŠ¥æ•°æ®..."):
                                        for s in selected_symbols:
                                            try:
                                                export_financials_single(s, str(sd_fi).replace("-",""), str(ed_fi).replace("-",""), funda_dir, selected_industry)
                                            except Exception as e:
                                                pass
                                    st.success("è´¢æŠ¥é‡‡é›†å®Œæˆï¼Œè¯·å†æ¬¡ç‚¹å‡»â€œè¿è¡Œè¡Œä¸šåˆ†æâ€")
                        
                        # è¿è¡Œè¡Œä¸šåˆ†æ
                        run_industry_btn = st.button("è¿è¡Œè¡Œä¸šåˆ†æ", key="run_industry_right", type="primary")
                        
                        if run_industry_btn:
                            weights = {"profitability": wp, "solvency": ws, "growth": wg, "return": wr}
                            tw = sum(weights.values())
                            if abs(tw - 1.0) > 1e-6 and tw > 0:
                                for k in list(weights.keys()):
                                    weights[k] = weights[k]/tw
                            
                            funda_dir_default = str((ROOT / "data" / "fundamentals").resolve())
                            funda_dir = Path(st.session_state.get("funda_dir", funda_dir_default))
                            
                            with st.spinner("æ­£åœ¨è®¡ç®—è¡Œä¸šè¯„åˆ†..."):
                                res = compute_industry_scoring(df_map, selected_industry, funda_dir, selected_symbols, weights)
                                
                                if res is not None and not res.empty:
                                  
                                    st.subheader("è¯„åˆ†ç»“æœ")
                                    df_show = res.reset_index()[["symbol","score_profitability","score_solvency","score_growth","score_return","composite_score"]].rename(columns={
                                        "symbol": "è¯åˆ¸ä»£ç ",
                                        "score_profitability": "ç›ˆåˆ©èƒ½åŠ›è¯„åˆ†",
                                        "score_solvency": "å¿å€ºèƒ½åŠ›è¯„åˆ†",
                                        "score_growth": "æˆé•¿æ€§è¯„åˆ†",
                                        "score_return": "æŠ•èµ„å›æŠ¥è¯„åˆ†",
                                        "composite_score": "ç»¼åˆè¯„åˆ†"
                                    })
                                    st.dataframe(df_show)
                                    
                                    st.subheader("è¡Œä¸šåŸºå‡†ä¸æ¨ªå‘åˆ†æ")
                                    all_dfs = []
                                    for sym in selected_symbols:
                                        try:
                                            df_i = fetch_resolved_df(st.session_state, sym, selected_industry, start_date, end_date, frequency)
                                            if not df_i.empty:
                                                df_i = df_i[["date","close"]].rename(columns={"close": sym})
                                                df_i["date"] = pd.to_datetime(df_i["date"])
                                                all_dfs.append(df_i)
                                        except Exception:
                                            pass
                                    pivot_close = None
                                    if all_dfs:
                                        from functools import reduce
                                        pivot_close = reduce(lambda l, r: pd.merge(l, r, on="date", how="outer"), all_dfs)
                                        pivot_close = pivot_close.set_index("date").sort_index()
                                    core_rows = []
                                    pe_vals = []
                                    pb_vals = []
                                    if pivot_close is not None and not pivot_close.empty:
                                        R = pivot_close.pct_change(fill_method=None).dropna().mean() * 252
                                        V = pivot_close.pct_change(fill_method=None).dropna().std() * np.sqrt(252)
                                        for sym in selected_symbols:
                                            ser = pd.to_numeric(pivot_close[sym], errors="coerce") if sym in pivot_close.columns else pd.Series(dtype=float)
                                            ret = float(compute_ann_return(ser)) if not ser.empty else 0.0
                                            vol = float(ser.pct_change(fill_method=None).dropna().std() * np.sqrt(252)) if not ser.empty else 0.0
                                            mdd = float(compute_max_drawdown(ser)) if not ser.empty else 0.0
                                            shp = float(compute_sharpe(ser)) if not ser.empty else 0.0
                                            funda_dir_default = str((ROOT / "data" / "fundamentals").resolve())
                                            fi_fp = Path(st.session_state.get("funda_dir", funda_dir_default)) / sym.replace(".","_") / "fina_indicator.csv"
                                            db_fp = Path(st.session_state.get("funda_dir", funda_dir_default)) / sym.replace(".","_") / "daily_basic.csv"
                                            pe = None; pb = None
                                            db = None
                                            if db_fp.exists():
                                                try:
                                                    db = pd.read_csv(db_fp)
                                                except Exception:
                                                    db = None
                                            if db is not None:
                                                db = db.sort_values(["trade_date"]) if "trade_date" in db.columns else db
                                                pe_cols_db = [c for c in ["pe","pe_ttm"] if c in db.columns]
                                                pb_cols_db = [c for c in ["pb","pb_mrq"] if c in db.columns]
                                                if pe_cols_db:
                                                    ser_pe = pd.to_numeric(db[pe_cols_db[0]], errors="coerce").dropna()
                                                    if not ser_pe.empty:
                                                        pe = float(ser_pe.iloc[-1])
                                                if pb_cols_db:
                                                    ser_pb = pd.to_numeric(db[pb_cols_db[0]], errors="coerce").dropna()
                                                    if not ser_pb.empty:
                                                        pb = float(ser_pb.iloc[-1])
                                            if (pe is None or pb is None) and fi_fp.exists():
                                                try:
                                                    fi = pd.read_csv(fi_fp)
                                                    fi = fi.sort_values(["end_date","ann_date"]) if "end_date" in fi.columns and "ann_date" in fi.columns else fi
                                                    pe_cols = [c for c in ["pe","pe_ttm","pe_basic","pe_circ"] if c in fi.columns]
                                                    pb_cols = [c for c in ["pb","pb_mrq"] if c in fi.columns]
                                                    if pe is None and pe_cols:
                                                        ser_pe = pd.to_numeric(fi[pe_cols[0]], errors="coerce").dropna()
                                                        if not ser_pe.empty:
                                                            pe = float(ser_pe.iloc[-1])
                                                    if pb is None and pb_cols:
                                                        ser_pb = pd.to_numeric(fi[pb_cols[0]], errors="coerce").dropna()
                                                        if not ser_pb.empty:
                                                            pb = float(ser_pb.iloc[-1])
                                                except Exception:
                                                    pass
                                            s_close = pd.to_numeric(pivot_close[sym], errors="coerce").dropna() if sym in pivot_close.columns else pd.Series(dtype=float)
                                            price_last = float(s_close.iloc[-1]) if not s_close.empty else None
                                            if pe is None and fi_fp.exists() and price_last is not None:
                                                try:
                                                    fi2 = pd.read_csv(fi_fp)
                                                    fi2 = fi2.sort_values(["end_date","ann_date"]) if "end_date" in fi2.columns and "ann_date" in fi2.columns else fi2
                                                    eps_cols = [c for c in ["eps","eps_basic","basic_eps","eps_ttm","eps_diluted"] if c in fi2.columns]
                                                    if eps_cols:
                                                        ser_eps = pd.to_numeric(fi2[eps_cols[0]], errors="coerce").dropna()
                                                        if not ser_eps.empty:
                                                            eps_last = float(ser_eps.iloc[-1])
                                                            if eps_last != 0:
                                                                pe = price_last / eps_last
                                                except Exception:
                                                    pass
                                            if pb is None and price_last is not None:
                                                bps = None
                                                if fi_fp.exists():
                                                    try:
                                                        fi3 = pd.read_csv(fi_fp)
                                                        fi3 = fi3.sort_values(["end_date","ann_date"]) if "end_date" in fi3.columns and "ann_date" in fi3.columns else fi3
                                                        bps_cols = [c for c in ["bps","net_asset_ps","net_assets_ps"] if c in fi3.columns]
                                                        if bps_cols:
                                                            ser_bps = pd.to_numeric(fi3[bps_cols[0]], errors="coerce").dropna()
                                                            if not ser_bps.empty:
                                                                bps = float(ser_bps.iloc[-1])
                                                    except Exception:
                                                        bps = None
                                                if bps is not None and bps != 0:
                                                    pb = price_last / bps
                                            if pe is not None: pe_vals.append(pe)
                                            if pb is not None: pb_vals.append(pb)
                                            core_rows.append({"symbol": sym, "æ”¶ç›Šç‡": ret, "æ³¢åŠ¨ç‡": vol, "æœ€å¤§å›æ’¤": mdd, "å¤æ™®æ¯”ç‡": shp, "PE": pe, "PB": pb})
                                        core_df = pd.DataFrame(core_rows).set_index("symbol")
                                        baseline_ret = float(R.mean()) if not R.empty else 0.0
                                        baseline_vol = float(V.mean()) if not V.empty else 0.0
                                        pe_med = float(pd.Series(pe_vals).median()) if pe_vals else None
                                        pb_med = float(pd.Series(pb_vals).median()) if pb_vals else None
                                        idx_norm = pivot_close.copy()
                                        for c in idx_norm.columns:
                                            try:
                                                s = pd.to_numeric(idx_norm[c], errors="coerce").dropna()
                                                if not s.empty and s.iloc[0] != 0:
                                                    idx_norm[c] = s / s.iloc[0]
                                            except Exception:
                                                pass
                                        ind_idx = idx_norm.mean(axis=1).dropna()
                                        mood = "éœ‡è¡"
                                        try:
                                            window = ind_idx.iloc[-90:] if ind_idx.shape[0] >= 90 else ind_idx
                                            change = float(window.iloc[-1]/window.iloc[0] - 1.0) if window.iloc[0] else 0.0
                                            if change > 0.05:
                                                mood = "æ™¯æ°”ä¸Šè¡ŒæœŸ"
                                            elif change < -0.05:
                                                mood = "æ™¯æ°”ä¸‹è¡ŒæœŸ"
                                        except Exception:
                                            mood = "éœ‡è¡"
                                        st.markdown(f"è¡Œä¸šåŸºå‡†ï¼šå¹³å‡å¹´åŒ–æ”¶ç›Šç‡ {baseline_ret:.2%}ï¼Œå¹³å‡æ³¢åŠ¨ç‡ {baseline_vol:.2%}ï¼Œä¼°å€¼ä¸­ä½æ•° PE={pe_med if pe_med is not None else 'N/A'}ï¼ŒPB={pb_med if pb_med is not None else 'N/A'}ï¼Œè¡Œä¸šæ•´ä½“ï¼š{mood}")
                                        tags_out = []
                                        for sym, row in core_df.iterrows():
                                            tg = generate_security_tags(float(row["æ”¶ç›Šç‡"]), float(row["æ³¢åŠ¨ç‡"]), float(row["æœ€å¤§å›æ’¤"]), float(row["å¤æ™®æ¯”ç‡"]), row["PE"] if not pd.isna(row["PE"]) else None, row["PB"] if not pd.isna(row["PB"]) else None, baseline_ret, baseline_vol)
                                            tags_out.append({"symbol": sym, "æ ‡ç­¾": "ã€".join(tg)})
                                        tags_df = pd.DataFrame(tags_out).set_index("symbol")
                                        core_df = core_df.join(tags_df, how="left")
                                        st.dataframe(core_df)
                                        
                                        st.subheader("ä¸ªè‚¡æ˜Ÿçº§æŠ¥å‘Š")
                                        def _to_0_100_series(s):
                                            s_num = pd.to_numeric(s, errors="coerce")
                                            clipped = s_num.clip(-3.0, 3.0)
                                            return (clipped + 3.0) / 6.0 * 100.0
                                        def _star(p):
                                            if p is None: return "N/A"
                                            if p >= 90: return "â­â­â­â­â­"
                                            if p >= 80: return "â­â­â­â­"
                                            if p >= 70: return "â­â­â­"
                                            if p >= 60: return "â­â­"
                                            return "â­"
                                        def _bucket(p, kind):
                                            if p is None: return "æ•°æ®ä¸è¶³"
                                            if kind == "profit":
                                                if p >= 80: return "èµšé’±èƒ½åŠ›å¾ˆå¼º"
                                                if p >= 60: return "èµšé’±èƒ½åŠ›ä¸é”™"
                                                if p >= 40: return "èµšé’±ä¸€èˆ¬"
                                                if p >= 20: return "èµšé’±åå¼±"
                                                return "èµšé’±èƒ½åŠ›å·®"
                                            if kind == "solvency":
                                                if p >= 80: return "è´¢åŠ¡éå¸¸ç¨³å¥"
                                                if p >= 60: return "è´¢åŠ¡å¥åº·"
                                                if p >= 40: return "å°šå¯æ¥å—"
                                                if p >= 20: return "å‹åŠ›åå¤§"
                                                return "å­˜åœ¨å¿å€ºé£é™©"
                                            if kind == "growth":
                                                if p >= 80: return "é«˜æˆé•¿å…¬å¸"
                                                if p >= 60: return "ç¨³å¥å¢é•¿"
                                                if p >= 40: return "å¢é•¿ä¸€èˆ¬"
                                                if p >= 20: return "å¢é•¿ä¹åŠ›"
                                                return "åŸºæœ¬ä¸å¢é•¿"
                                            if kind == "return":
                                                if p >= 80: return "å›æŠ¥éå¸¸å‹å¥½"
                                                if p >= 60: return "å›æŠ¥è¾ƒå¥½"
                                                if p >= 40: return "å›æŠ¥ä¸€èˆ¬"
                                                if p >= 20: return "å›æŠ¥åä½"
                                                return "å›æŠ¥ä¸ç†æƒ³"
                                            return ""
                                        def _map_fin_profit(v):
                                            if v is None or pd.isna(v): return None
                                            x = float(v)
                                            if x >= 0:
                                                return 60.0 + min(x, 3.0) / 3.0 * 40.0
                                            return max(0.0, 60.0 + max(x, -3.0) / 3.0 * 60.0)
                                        def _map_fin_solvency(v):
                                            if v is None or pd.isna(v): return None
                                            x = float(v)
                                            if x >= 0:
                                                return 80.0 + min(x, 3.0) / 3.0 * 20.0
                                            if x >= -10.0:
                                                return 60.0 + (x + 10.0) / 10.0 * 20.0
                                            if x >= -30.0:
                                                return 30.0 + (x + 30.0) / 20.0 * 30.0
                                            return max(0.0, 0.0 + min(x, -60.0) / -30.0 * 30.0)
                                        def _map_fin_growth(v):
                                            if v is None or pd.isna(v): return None
                                            x = float(v)
                                            if x <= -1.0: return 0.0
                                            if x <= 0.0: return 0.0
                                            return min(100.0, x * 100.0)
                                        def _map_fin_return(v):
                                            return _map_fin_profit(v)
                                        def _percentile_score(series, val, reverse=False):
                                            s = pd.to_numeric(series, errors="coerce").dropna()
                                            if s.empty or val is None or pd.isna(val): return None
                                            import numpy as np
                                            arr = s.values
                                            rank = float((arr <= val).sum()) / float(arr.size) * 100.0 if not reverse else float((arr >= val).sum()) / float(arr.size) * 100.0
                                            return rank
                                        def _valuation_score(val, med):
                                            if val is None or pd.isna(val) or med is None: return None
                                            m = float(med)
                                            if m == 0.0: return None
                                            d = abs(float(val) - m) / abs(m)
                                            sc = 100.0 - min(100.0, d * 100.0)
                                            return max(0.0, sc)
                                        tag_map = {"â†‘ä¸­æ€§": 70.0, "â–³é«˜æ³¢åŠ¨ä¸ç¨³å®š": 40.0}
                                        Pp_raw = res["score_profitability"]
                                        Ps_raw = res["score_solvency"]
                                        Pg_raw = res["score_growth"]
                                        Pr_raw = res["score_return"]
                                        Pp = Pp_raw.apply(_map_fin_profit)
                                        Ps = Ps_raw.apply(_map_fin_solvency)
                                        Pg = Pg_raw.apply(_map_fin_growth)
                                        Pr = Pr_raw.apply(_map_fin_return)
                                        Pc = pd.Series(dtype=float)
                                        for sym in res.index.tolist():
                                            p = Pp.loc[sym] if sym in Pp.index else None
                                            ssv = Ps.loc[sym] if sym in Ps.index else None
                                            g = Pg.loc[sym] if sym in Pg.index else None
                                            r = Pr.loc[sym] if sym in Pr.index else None
                                            ret_v = float(core_df.loc[sym, "æ”¶ç›Šç‡"]) if sym in core_df.index and not pd.isna(core_df.loc[sym, "æ”¶ç›Šç‡"]) else None
                                            shp_v = float(core_df.loc[sym, "å¤æ™®æ¯”ç‡"]) if sym in core_df.index and not pd.isna(core_df.loc[sym, "å¤æ™®æ¯”ç‡"]) else None
                                            mdd_v = float(core_df.loc[sym, "æœ€å¤§å›æ’¤"]) if sym in core_df.index and not pd.isna(core_df.loc[sym, "æœ€å¤§å›æ’¤"]) else None
                                            vol_v = float(core_df.loc[sym, "æ³¢åŠ¨ç‡"]) if sym in core_df.index and not pd.isna(core_df.loc[sym, "æ³¢åŠ¨ç‡"]) else None
                                            pe_v = core_df.loc[sym, "PE"] if sym in core_df.index else None
                                            pb_v = core_df.loc[sym, "PB"] if sym in core_df.index else None
                                            ret_sc = _percentile_score(core_df["æ”¶ç›Šç‡"], ret_v, reverse=False)
                                            shp_sc = _percentile_score(core_df["å¤æ™®æ¯”ç‡"], shp_v, reverse=False)
                                            mdd_sc = _percentile_score(abs(core_df["æœ€å¤§å›æ’¤"]), abs(mdd_v), reverse=True)
                                            vol_sc = _percentile_score(core_df["æ³¢åŠ¨ç‡"], vol_v, reverse=True)
                                            pe_sc = _valuation_score(pe_v, pe_med) if pe_med is not None else None
                                            pb_sc = _valuation_score(pb_v, pb_med) if pb_med is not None else None
                                            fin_list = [p, ssv, g, r]
                                            fin_avg = float(pd.Series([x for x in fin_list if x is not None]).mean()) if any(x is not None for x in fin_list) else None
                                            mkt_list = [ret_sc, shp_sc, mdd_sc, vol_sc]
                                            mkt_avg = float(pd.Series([x for x in mkt_list if x is not None]).mean()) if any(x is not None for x in mkt_list) else None
                                            val_list = [pe_sc, pb_sc]
                                            val_avg = float(pd.Series([x for x in val_list if x is not None]).mean()) if any(x is not None for x in val_list) else None
                                            tag_raw = core_df.loc[sym, "æ ‡ç­¾"] if sym in core_df.index and "æ ‡ç­¾" in core_df.columns else None
                                            tag_score = None
                                            if tag_raw and isinstance(tag_raw, str):
                                                tags_arr = [t.strip() for t in tag_raw.split("ã€") if t.strip()]
                                                if tags_arr:
                                                    tag_vals = [tag_map.get(t, 50.0) for t in tags_arr]
                                                    tag_score = float(pd.Series(tag_vals).mean())
                                            parts = []
                                            if fin_avg is not None: parts.append(fin_avg * 0.5)
                                            if mkt_avg is not None: parts.append(mkt_avg * 0.3)
                                            if val_avg is not None: parts.append(val_avg * 0.1)
                                            if tag_score is not None: parts.append(tag_score * 0.1)
                                            comp = float(pd.Series(parts).sum()) if parts else None
                                            Pc.loc[sym] = comp if comp is not None else np.nan
                                            try:
                                                comp_raw_val = float(res["composite_score"].loc[sym])
                                                if pd.isna(comp_raw_val):
                                                # safeguard for NaN
                                                    comp_raw = None
                                                else:
                                                    comp_raw = comp_raw_val
                                            except Exception:
                                                comp_raw = None
                                            star = _star(comp)
                                            header_txt = f"{sym} | ç»¼åˆæ˜Ÿçº§ï¼š{star}"
                                            if comp_raw is not None:
                                                header_txt += f" | ç»¼åˆåˆ†ï¼ˆæ ‡å‡†åŒ–ï¼‰ï¼š{comp_raw:.2f}"
                                            def _explain(kind, tag):
                                                if kind == "profit":
                                                    if tag == "èµšé’±èƒ½åŠ›å¾ˆå¼º": return "ROE/EPSä¸åˆ©æ¶¦ç‡æ˜¾è‘—é¢†å…ˆï¼Œç›ˆåˆ©ç»“æ„ç¨³å¥ï¼Œå…·æœ‰æŒç»­æ€§ã€‚"
                                                    if tag == "èµšé’±èƒ½åŠ›ä¸é”™": return "ç›ˆåˆ©ç¨³å®šï¼Œç›ˆåˆ©è´¨é‡è‰¯å¥½ï¼Œå…·å¤‡ä¸€å®šæŠ¤åŸæ²³ä¸æˆæœ¬æ§åˆ¶èƒ½åŠ›ã€‚"
                                                    if tag == "èµšé’±ä¸€èˆ¬": return "å¤„äºè¡Œä¸šä¸­æ¸¸ï¼Œåˆ©æ¶¦ç‡éšå‘¨æœŸæ³¢åŠ¨ï¼Œéœ€å…³æ³¨ææ•ˆä¸äº§å“ç»“æ„ä¼˜åŒ–ã€‚"
                                                    if tag == "èµšé’±åå¼±": return "ç›ˆåˆ©æ°´å¹³åä½æˆ–ä¸ç¨³å®šï¼Œå»ºè®®è°¨æ…è§‚å¯ŸåŸºæœ¬é¢æ”¹å–„ä¿¡å·ã€‚"
                                                    return "çŸ­æœŸç›ˆåˆ©æ‰¿å‹æˆ–å•†ä¸šæ¨¡å¼å¾…éªŒè¯ï¼Œéœ€é™ä½é¢„æœŸå¹¶æ§åˆ¶ä»“ä½ã€‚"
                                                if kind == "solvency":
                                                    if tag == "è´¢åŠ¡éå¸¸ç¨³å¥": return "è´Ÿå€ºå¯æ§ï¼Œç°é‡‘æµå……è¶³ï¼ŒæµåŠ¨/é€ŸåŠ¨æ¯”ç‡åœ¨å®‰å…¨åŒºé—´ï¼ŒæŠ—é£é™©èƒ½åŠ›å¼ºã€‚"
                                                    if tag == "è´¢åŠ¡å¥åº·": return "è´Ÿå€ºç»“æ„åˆç†ï¼Œå¿å€ºå‹åŠ›ä¸å¤§ï¼Œèµ„é‡‘å‘¨è½¬æ­£å¸¸ã€‚"
                                                    if tag == "å°šå¯æ¥å—": return "å¿å€ºèƒ½åŠ›ä¸€èˆ¬ï¼Œéœ€å…³æ³¨è´Ÿå€ºä¸ç°é‡‘æµçš„å˜åŒ–è¶‹åŠ¿ã€‚"
                                                    if tag == "å‹åŠ›åå¤§": return "è´Ÿå€ºç‡åé«˜æˆ–å¿ä»˜èƒ½åŠ›èµ°å¼±ï¼Œå»ºè®®é™é£é™©æš´éœ²ã€‚"
                                                    return "è´¢åŠ¡é£é™©è¾ƒé«˜ï¼Œå»ºè®®å›é¿æˆ–ç­‰å¾…è´¢åŠ¡ç»“æ„æ˜¾è‘—æ”¹å–„ã€‚"
                                                if kind == "growth":
                                                    if tag == "é«˜æˆé•¿å…¬å¸": return "æ”¶å…¥ä¸åˆ©æ¶¦é«˜é€Ÿæ‰©å¼ ï¼Œå¤–å»¶ä¸å†…ç”Ÿå¢é•¿å‡å…·å¤‡åŠ¨åŠ›ã€‚"
                                                    if tag == "ç¨³å¥å¢é•¿": return "å¢é€Ÿé«˜äºè¡Œä¸šå¹³å‡ï¼Œå¢é•¿è´¨é‡è¾ƒå¥½ã€‚"
                                                    if tag == "å¢é•¿ä¸€èˆ¬": return "å¢é•¿æ¥è¿‘è¡Œä¸šå¹³å‡ï¼Œéœ€å¯»æ‰¾æ–°äº§å“æˆ–æ¸ é“æå‡åŠ¨èƒ½ã€‚"
                                                    if tag == "å¢é•¿ä¹åŠ›": return "æ¥è¿‘åœæ»ï¼Œéœ€é€šè¿‡é™æœ¬å¢æ•ˆæˆ–ç»“æ„è°ƒæ•´æ”¹å–„ã€‚"
                                                    return "æˆç†Ÿæˆ–è¡°é€€é˜¶æ®µï¼Œå¢é•¿å¼¹æ€§å¼±ï¼Œç­–ç•¥ä»¥ä¼°å€¼ä¸åˆ†çº¢ä¸ºä¸»ã€‚"
                                                if kind == "return":
                                                    if tag == "å›æŠ¥éå¸¸å‹å¥½": return "é«˜ROEä¸åˆ†çº¢æ°´å¹³ï¼Œèµ„æœ¬å›æŠ¥çªå‡ºï¼Œé•¿æœŸé…ç½®å‹å¥½ã€‚"
                                                    if tag == "å›æŠ¥è¾ƒå¥½": return "è‚¡ä¸œå›æŠ¥è¾ƒä¼˜ï¼Œå…·å¤‡ä¸­é•¿æœŸæŒæœ‰ä»·å€¼ã€‚"
                                                    if tag == "å›æŠ¥ä¸€èˆ¬": return "å›æŠ¥æ›´å¤šæ¥è‡ªè‚¡ä»·æ³¢åŠ¨ï¼Œéœ€ç»“åˆä¼°å€¼ä¸è¶‹åŠ¿æ‹©æ—¶ã€‚"
                                                    if tag == "å›æŠ¥åä½": return "èµ„é‡‘å ç”¨æ•ˆç‡è¾ƒä½ï¼Œéœ€ç­‰å¾…åŸºæœ¬é¢æ”¹å–„æˆ–ä¼°å€¼åˆ‡æ¢ã€‚"
                                                    return "é•¿æœŸå›æŠ¥ä¸ç†æƒ³ï¼Œå»ºè®®è°¨æ…æˆ–å›é¿ã€‚"
                                                return ""
                                            with st.expander(header_txt, expanded=False):
                                                bp = _bucket(p, 'profit')
                                                st.markdown(f"ğŸ“ˆ ç›ˆåˆ©èƒ½åŠ›ï¼š{bp}")
                                                st.caption(_explain('profit', bp))
                                                bs_ = _bucket(ssv, 'solvency')
                                                st.markdown(f"ğŸ›¡ï¸ è´¢åŠ¡ç»“æ„ï¼š{bs_}")
                                                st.caption(_explain('solvency', bs_))
                                                bg_ = _bucket(g, 'growth')
                                                st.markdown(f"ğŸŒ± æˆé•¿æ€§ï¼š{bg_}")
                                                st.caption(_explain('growth', bg_))
                                                br_ = _bucket(r, 'return')
                                                st.markdown(f"ğŸ’° æŠ•èµ„å›æŠ¥ï¼š{br_}")
                                                st.caption(_explain('return', br_))
                                                if comp is not None:
                                                    one_line = "ä¼˜ç§€ï¼Œå„é¡¹è¡¨ç°å‡è¡¡ä¼˜å¼‚" if comp >= 90 else ("è‰¯å¥½ï¼Œå¤šæ•°æŒ‡æ ‡è¡¨ç°çªå‡º" if comp >= 80 else ("ä¸­ç­‰ï¼Œç¬¦åˆè¡Œä¸šå¹³å‡æ°´å¹³" if comp >= 70 else ("ä¸€èˆ¬ï¼Œå­˜åœ¨æ˜æ˜¾çŸ­æ¿" if comp >= 60 else "è¾ƒå·®ï¼Œå¤šæŒ‡æ ‡è¡¨ç°ä¸ä½³")))
                                                    st.markdown(f"ğŸ§  ç»¼åˆåˆ¤æ–­ï¼š{one_line}")
                                                    if comp >= 90:
                                                        st.caption("å»ºè®®ï¼šæ ¸å¿ƒæŒä»“ï¼Œåˆ†æ•£æ§åˆ¶è¡Œä¸šä¸é£æ ¼é£é™©ï¼Œå…³æ³¨ä¼°å€¼ä¸å›æ’¤çº¦æŸã€‚")
                                                    elif comp >= 80:
                                                        st.caption("å»ºè®®ï¼šé‡ç‚¹å…³æ³¨ï¼Œç»“åˆä¼°å€¼åˆ†æ‰¹é…ç½®ï¼Œä¿æŒæ­¢æŸçºªå¾‹ã€‚")
                                                    elif comp >= 70:
                                                        st.caption("å»ºè®®ï¼šä¸­æ€§é…ç½®ï¼Œæ‹©æœºå‚ä¸ï¼Œéœ€ä¸è¡Œä¸šç¯å¢ƒä¸ä¼°å€¼å…±æŒ¯ã€‚")
                                                    elif comp >= 60:
                                                        st.caption("å»ºè®®ï¼šè½»ä»“è§‚å¯Ÿï¼Œå­˜åœ¨çŸ­æ¿ï¼Œç­‰å¾…åŸºæœ¬é¢æˆ–è¡Œä¸šå‚¬åŒ–æ”¹å–„åå†æé«˜æƒé‡ã€‚")
                                                    else:
                                                        st.caption("å»ºè®®ï¼šå›é¿æˆ–ä½æƒé‡æŒæœ‰ï¼Œé£é™©è¾ƒé«˜ï¼Œéœ€æ˜ç¡®æ”¹å–„ä¿¡å·åå†è¯„ä¼°ã€‚")
                                                try:
                                                    dj = []
                                                    if isinstance(bp, str): dj.append(f"ç›ˆåˆ©ï¼š{bp}")
                                                    if isinstance(bs_, str): dj.append(f"è´¢åŠ¡ï¼š{bs_}")
                                                    if isinstance(bg_, str): dj.append(f"æˆé•¿ï¼š{bg_}")
                                                    if isinstance(br_, str): dj.append(f"å›æŠ¥ï¼š{br_}")
                                                    comp_txt = f"ç»¼åˆæ˜Ÿçº§ï¼š{star}ï¼Œç»¼åˆå¾—åˆ†ï¼š{comp:.1f}" if comp is not None else f"ç»¼åˆæ˜Ÿçº§ï¼š{star}"
                                                    base_txt = "ï¼›".join(dj)
                                                    xj_text = f"{sym}ï¼ˆ{selected_industry}ï¼‰ç»¼åˆç‚¹è¯„ï¼š{comp_txt}ï¼›{base_txt}ã€‚"
                                                    advisor_text(xj_text)
                                                except Exception:
                                                    pass
                                       
                                        st.subheader("æ’åä¸æ¢¯é˜Ÿ")
                                        rank_profit = res["score_profitability"].sort_values(ascending=False)
                                        rank_risk = res["score_solvency"].sort_values(ascending=False)
                                        rank_comp = res["composite_score"].sort_values(ascending=False)
                                        def _tiers(series: pd.Series) -> pd.DataFrame:
                                            n = series.shape[0]
                                            q1 = int(max(1, round(n*0.33)))
                                            q2 = int(max(1, round(n*0.66)))
                                            labels = []
                                            for i, s in enumerate(series.index):
                                                if i < q1:
                                                    labels.append("ç¬¬ä¸€æ¢¯é˜Ÿ")
                                                elif i < q2:
                                                    labels.append("ç¬¬äºŒæ¢¯é˜Ÿ")
                                                else:
                                                    labels.append("ç¬¬ä¸‰æ¢¯é˜Ÿ")
                                            return pd.DataFrame({"symbol": series.index, "tier": labels}).set_index("symbol")
                                        tiers_df = _tiers(rank_comp)
                                        st.markdown(f"æ”¶ç›Šèƒ½åŠ›æ’åï¼š{', '.join(rank_profit.index.tolist())}")
                                        st.markdown(f"é£é™©æ§åˆ¶èƒ½åŠ›æ’åï¼š{', '.join(rank_risk.index.tolist())}")
                                        st.markdown(f"ç»¼åˆè¯„åˆ†æ’åï¼š{', '.join(rank_comp.index.tolist())}")
                                        t_groups = tiers_df.groupby("tier").apply(lambda x: ", ".join(x.index.tolist()))
                                        for tier, members in t_groups.items():
                                            st.markdown(f"{tier}ï¼š{members}")
                                        st.subheader("è¡Œä¸šç›¸å…³æ€§ä¸åˆ†æ•£æ€§")
                                        corr = compute_corr_matrix(pivot_close)
                                        fig_corr = plot_corr_heatmap(corr)
                                        st.plotly_chart(fig_corr, width="stretch")
                                        st.caption(f"å¹³å‡ç›¸å…³æ€§ {corr.mean().mean():.3f}ï¼Œç›¸å…³æ€§è¶Šä½åˆ†æ•£æ•ˆæœè¶Šå¥½ã€‚")
                                        rets = pivot_close.pct_change(fill_method=None).dropna()
                                        if not rets.empty:
                                            port_ret = rets.mean(axis=1)
                                            port_vol = float(port_ret.std() * np.sqrt(252))
                                            ind_vol_mean = float(rets.std().mean() * np.sqrt(252))
                                            drop_pct = float((ind_vol_mean - port_vol) / ind_vol_mean) if ind_vol_mean else 0.0
                                            avgc = float(corr.mean().mean())
                                            if avgc >= 0.7:
                                                tag = "é«˜åº¦è”åŠ¨å‹"
                                            elif avgc >= 0.3:
                                                tag = "å¼±è”åŠ¨å‹"
                                            elif avgc > 0.0:
                                                tag = "åˆ†æ•£é…ç½®å‹"
                                            else:
                                                tag = "å¯¹å†²å‹"
                                            if drop_pct >= 0.10:
                                                eff = "åˆ†æ•£æ•ˆæœæ˜¾è‘—"
                                            elif drop_pct >= 0.05:
                                                eff = "åˆ†æ•£æ•ˆæœä¸€èˆ¬"
                                            else:
                                                eff = "åˆ†æ•£æ•ˆæœæœ‰é™"
                                            st.markdown(f"å°ç»“è®ºï¼šè¡Œä¸šæ•´ä½“ä¸ºã€Œ{tag}ã€ï¼Œç­‰æƒç»„åˆæ³¢åŠ¨ç‡è¾ƒä¸ªè‚¡å¹³å‡é™ä½ {drop_pct*100:.1f}%ï¼ˆ{eff}ï¼‰ã€‚")
                                            if avgc >= 0.5:
                                                st.markdown("æç¤ºï¼šå¹³å‡ç›¸å…³æ€§åé«˜ï¼Œå»ºè®®å¼•å…¥ä½ç›¸å…³è¡Œä¸šä»¥æå‡ç»„åˆç¨³å®šæ€§ã€‚")
                                            st.caption("äººè¯ï¼šå¦‚æœè€æ˜¯ä¸€èµ·æ¶¨ã€ä¸€èµ·è·Œï¼Œé‚£åˆ†æ•£å°±ä¸æ˜æ˜¾ï¼›èµ°åŠ¿è¶Šä¸ä¸€æ ·ï¼Œç”šè‡³åç€èµ°ï¼Œæ‰æ›´èƒ½å¸®ä½ é™é£é™©ã€‚")
                                        industry_summary = {
                                            "industry": selected_industry,
                                            "selected_stocks": selected_symbols,
                                            "weights": weights,
                                            "scores": res.to_dict(orient="index"),
                                            "baseline": {"avg_return": baseline_ret, "avg_volatility": baseline_vol, "pe_median": pe_med, "pb_median": pb_med, "mood": mood},
                                            "core_metrics": core_df.to_dict(orient="index")
                                        }
                                    else:
                                        industry_summary = {
                                            "industry": selected_industry,
                                            "selected_stocks": selected_symbols,
                                            "weights": weights,
                                            "scores": res.to_dict(orient="index")
                                        }
                                    # å¤§æ¨¡å‹åˆ†æåŒºåŸŸ
                                    st.divider()
                                    st.subheader("ğŸ¤– å°é‡‘çš„è¡Œä¸šåˆ†æ")
                                    
                                    # ç”Ÿæˆè¡Œä¸šåˆ†ææŠ¥å‘Š
                                    
                                    
                                    # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆè¡Œä¸šåˆ†æ
                                    try:
                                        from src.conclusion import generate_industry_analysis
                                        industry_analysis = generate_industry_analysis(industry_summary)
                                        
                                        with st.expander("è¡Œä¸šåˆ†ææŠ¥å‘Š", expanded=True):
                                            st.markdown(industry_analysis)
                                        
                                        # å¯¼å‡ºåŠŸèƒ½
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            buf = io.StringIO()
                                            res.to_csv(buf, index=True, encoding="utf-8-sig")
                                            st.download_button("å¯¼å‡ºè¯„åˆ†CSV", buf.getvalue(), 
                                                             file_name=f"{selected_industry}_scores.csv", 
                                                             mime="text/csv")
                                        with col2:
                                            st.download_button("å¯¼å‡ºè¡Œä¸šåˆ†ææŠ¥å‘Š", industry_analysis, 
                                                             file_name=f"{selected_industry}_analysis.txt", 
                                                             mime="text/plain")
                                        
                                        # ç”Ÿæˆä¸ªè‚¡æŠ¥å‘Š
                                        st.subheader("ä¸ªè‚¡è¯¦ç»†æŠ¥å‘Š")
                                        all_txts = []
                                        for s in res.index:
                                            m = compute_symbol_metrics(funda_dir / s.replace('.', '_'))
                                            txt = generate_text_report(s, m)
                                            all_txts.append(txt)
                                            with st.expander(f"{s} è¯¦ç»†æŠ¥å‘Š"):
                                                st.markdown(txt)
                                        
                                        if all_txts:
                                            st.download_button("å¯¼å‡ºæ‰€æœ‰æŠ¥å‘Š", "\n\n".join(all_txts), 
                                                             file_name=f"{selected_industry}_reports.txt", 
                                                             mime="text/plain")
                                        
                                    except Exception as e:
                                        st.warning(f"å¤§æ¨¡å‹åˆ†æç”Ÿæˆå¤±è´¥: {str(e)}")
                                        # ç”ŸæˆåŸºæœ¬æŠ¥å‘Š
                                        st.info("ç”ŸæˆåŸºæœ¬åˆ†ææŠ¥å‘Š...")
                                        all_txts = []
                                        for s in res.index:
                                            m = compute_symbol_metrics(funda_dir / s.replace('.', '_'))
                                            txt = generate_text_report(s, m)
                                            all_txts.append(txt)
                                            with st.expander(f"{s} æŠ¥å‘Š"):
                                                st.markdown(txt)
                                        
                                        if all_txts:
                                            st.download_button("å¯¼å‡ºæŠ¥å‘Šæ–‡æœ¬", "\n\n".join(all_txts), 
                                                             file_name=f"{selected_industry}_reports.txt", 
                                                             mime="text/plain")
                                else:
                                    st.warning("è¡Œä¸šè´¢æŠ¥æ•°æ®ä¸è¶³ï¼Œè¯·å…ˆåœ¨ä¾§è¾¹æ æ‰§è¡Œ'æ‰¹é‡é‡‡é›†è´¢æŠ¥(å››è¡¨)'")
                else:
                    st.warning("è¯¥è¡Œä¸šä¸‹æ²¡æœ‰è¯åˆ¸æ•°æ®")
            else:
                st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ è¡Œä¸šæ˜ å°„æ–‡ä»¶")
        else:
            st.info("è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ è¡Œä¸šæ˜ å°„æ–‡ä»¶ä»¥è·å–è¡Œä¸šåˆ—è¡¨")
    else:
        pass
